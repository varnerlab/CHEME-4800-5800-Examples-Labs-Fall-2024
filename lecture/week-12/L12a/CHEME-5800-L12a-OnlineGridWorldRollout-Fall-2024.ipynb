{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394ceef2-328a-476f-b5ea-1b8d80984b2b",
   "metadata": {},
   "source": [
    "## Lab 11d: Online Planning in the Lava Grid World\n",
    "This lab will familiarize students with the `rollout` solution of a `two-dimensional` navigation problem, i.e., a lava world [roomba](https://www.irobot.com) problem. We'll use a `rollout` approach, a random policy for exploring the world. See `Algorithm 9.1` of the [Decisions Book](https://algorithmsbook.com).\n",
    "\n",
    "### Problem\n",
    "You have a [roomba](https://www.irobot.com) that has finished cleaning the kitchen floor and needs to return to its charging station. However, between your kitchen floor and the `charging station` (safety), there are one or more `lava pits` (destruction for the [roomba](https://www.irobot.com)). This is an example of a two-dimensional grid-world navigational decision task. \n",
    "\n",
    "### Objectives and tasks\n",
    "This example will familiarize students with using `rollout` for solving a two-dimensional grid-world navigation task, the role of the discount factor $\\gamma$. In particular, we will:\n",
    "\n",
    "* __Task 1__: Build a `n` $\\times$ `n` world model with two lava pits and a charging station.\n",
    "* __Task 2__: Generate the components of the MDP problem \n",
    "* __Task 3__: Develop on online planning solution by implementing a `rollout(...)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa3f28-dd00-495c-abc9-5058991a28ad",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We set up the computational environment by including [the `Include. jl` file](Include.jl) using [the `include(...)` method](https://docs.julialang.org/en/v1/base/base/#Base.include). The [`Include.jl` file](Include.jl) loads external packages and functions we will use in these examples. \n",
    "* For additional information on functions and types used in this example, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48e9efe-9565-402a-84aa-cc4f194930cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a/Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dbe5a8-b12d-47db-b310-f6138d0a5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "function rbf(x::Tuple{Int,Int},y::Tuple{Int,Int}; σ = 1.0)::Float64\n",
    "    d = sqrt((x[1] - y[1])^2 + (x[2] - y[2])^2);\n",
    "    return exp(-d/(2*σ^2))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57d202-434b-42ba-adef-0a3842a0ee4b",
   "metadata": {},
   "source": [
    "## Task 1: Build the world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's setup the data for the world, setup the states, actions, rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` that are avialble to the agent and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states, and setup the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564069a4-b30f-480f-83e3-7f72a3e651f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 10\n",
    "number_of_cols = 10\n",
    "nactions = 4;\n",
    "γ = 0.1;\n",
    "number_of_random_steps = 64;\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "𝒮 = range(1,stop=nstates,step=1) |> collect;\n",
    "𝒜 = range(1,stop=nactions,step=1) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fab8b7-67fe-4b3d-9cfa-a5c863540c22",
   "metadata": {},
   "source": [
    "Next, we'll set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. We only need to put `non-default` reward values in the reward dictionary (we'll add default values to the other locations later). Lastly, let's put the locations on the grid that are `absorbing`, meaning the charging station or lava pits in your living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f17787-9756-424c-8232-7cfdf4931a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup rewards -\n",
    "lava_reward = -1000.0;\n",
    "charging_reward = 100.0;\n",
    "softwall_reward = -2000.0;\n",
    "σ = 1.0;\n",
    "default_reward = -0.1;\n",
    "charging_station_coordinates = (1,3);\n",
    "\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(2,2)] = lava_reward # lava in the (2,2) square \n",
    "rewards[(4,4)] = lava_reward # lava in the (4,4) square\n",
    "rewards[(9,7)] = lava_reward # lava in the (9,7) square\n",
    "rewards[charging_station_coordinates] = charging_reward    # charging station square\n",
    "\n",
    "# walls?\n",
    "soft_wall_set = Set{Tuple{Int,Int}}(); # none for now ...\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (2,2));\n",
    "push!(absorbing_state_set, charging_station_coordinates);\n",
    "push!(absorbing_state_set, (4,4));\n",
    "push!(absorbing_state_set, (9,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5d9f4-06dc-4dc3-8a74-15c999ea95ea",
   "metadata": {},
   "source": [
    "### Reward shaping\n",
    "In cases where rewards are sparse, there are only a few $(s, a)$ pairs that lead to non-zero rewards. This is an issue because reinforcement learning algorithms, e.g., `Q-learning,` behave randomly initially when the action-value function $Q(s, a)$ is unknown. \n",
    "\n",
    "* [Reward shaping](https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.htmlhttps://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html) is an approach to address this issue, by modifying the reward function to promote behavior that we think will move us closer to the goal state, e.g., the `charging_station.` There are different approaches to this. We'll use a [radial basis kernel function](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) to radiate charging station rewards from the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f62c16-513f-445b-8d00-d1d5d9715ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some shaping?\n",
    "is_reward_shaping_on = true;\n",
    "if (is_reward_shaping_on == true)\n",
    "    for s in 𝒮\n",
    "        for s′ in 𝒮\n",
    "            coordinate = (s,s′);\n",
    "            if (haskey(rewards, coordinate) == false && in(coordinate,soft_wall_set) == false && \n",
    "                    in(coordinate,absorbing_state_set) == false)\n",
    "                rewards[coordinate] = default_reward + charging_reward*rbf(coordinate, charging_station_coordinates, σ = σ);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d5829-e0bb-488c-b2c4-05d4cc6aafa6",
   "metadata": {},
   "source": [
    "Finally, we can build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* We must pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ddd027-c480-4d63-a5b9-aa122e3aebb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "930c2a45-9248-435a-874f-d052a565f1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.states[charging_station_coordinates] |> i -> world.rewards[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80780e08-bac5-4b80-b73d-cc5c6dbe4bd7",
   "metadata": {},
   "source": [
    "## Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s′, a)`. Let's construct these from our grid world model instance, starting with the reward function `R(s, a)`:\n",
    "\n",
    "### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Δ`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13203c34-fd3e-4b19-8f48-825ce21d1460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×4 Matrix{Float64}:\n",
       " -50000.0           32.5922    -50000.0           60.5531\n",
       " -50000.0        -1000.0           36.6879       100.0\n",
       " -50000.0           60.5531        60.5531        60.5531\n",
       " -50000.0           49.2069       100.0           36.6879\n",
       " -50000.0           32.5922        60.5531        22.213\n",
       " -50000.0           20.4741        36.6879        13.4335\n",
       " -50000.0           12.6256        22.213          8.1085\n",
       " -50000.0            7.712         13.4335         4.87871\n",
       " -50000.0            4.67689        8.1085         2.91974\n",
       " -50000.0            2.81432        4.87871   -50000.0\n",
       "     36.6879        24.2117    -50000.0        -1000.0\n",
       "     60.5531        32.5922        32.5922        60.5531\n",
       "    100.0           36.6879     -1000.0           49.2069\n",
       "      ⋮                                       \n",
       "      0.895409       0.347913       0.794205       0.391692\n",
       "      0.60852        0.234303       0.573795  -50000.0\n",
       "      1.51941   -50000.0       -50000.0            0.980558\n",
       "      1.67543   -50000.0            0.895409       1.0109\n",
       "      1.73156   -50000.0            0.980558       0.980558\n",
       "      1.67543   -50000.0            1.0109         0.895409\n",
       "      1.51941   -50000.0            0.980558       0.770884\n",
       "      1.29538   -50000.0            0.895409       0.626688\n",
       "  -1000.0       -50000.0            0.770884       0.481209\n",
       "      0.794205  -50000.0            0.626688       0.347913\n",
       "      0.573795  -50000.0            0.481209       0.234303\n",
       "      0.391692  -50000.0            0.347913  -50000.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ∈ 𝒮\n",
    "    for a ∈ 𝒜\n",
    "        \n",
    "        Δ = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            if (haskey(rewards, new_position) == true)\n",
    "                R[s,a] = rewards[new_position];\n",
    "            else\n",
    "                R[s,a] = -1.0;\n",
    "            end\n",
    "        else\n",
    "            R[s,a] = -50000.0; # we are off the grid, big negative penalty\n",
    "        end\n",
    "    end\n",
    "end\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d472-a0c9-494b-b356-a2eb1cdd1b4e",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Δ`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s′,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b087127-75c8-40c6-b416-4ac41bd8081b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ∈ 𝒜\n",
    "    \n",
    "    Δ = world.moves[a];\n",
    "    \n",
    "    for s ∈ 𝒮\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s′ = world.states[new_position];\n",
    "            T[s, s′,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bd54d-4a91-4431-bddb-b932b6654452",
   "metadata": {},
   "source": [
    "Finally, we construct an instance of the `MyMDPProblemModel` which encodes the data required to solve the MDP problem.\n",
    "* We must pass the states `𝒮`, the actions `𝒜`, the transition matrix `T`, the reward matrix `R`, and the discount factor `γ` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe11305-9b82-4def-925c-99607990d6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `VLDecisionsPackage` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `VLDecisionsPackage` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[19]:1"
     ]
    }
   ],
   "source": [
    "m = build(MyMDPProblemModel, (𝒮 = 𝒮, 𝒜 = 𝒜, T = T, R = R, γ = γ));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0aa32-4847-4ef5-999d-f403fea2ce11",
   "metadata": {},
   "source": [
    "## Task 3: Online planning solution\n",
    "First, let's set the `depth` that are going to explore, i.e., how many steps are we going to take when exploring each state `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33d5d4de-39e8-4e24-a505-614413855418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = number_of_random_steps;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b503d3-be44-4dcf-9e20-5194dbb0ef5f",
   "metadata": {},
   "source": [
    "Next, let's implement three functions:\n",
    "\n",
    "> The `myrandpolicy(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int) -> Int` function takes a `MyMDPProblemModel` instance, a `MyRectangularGridWorldModel` instance and the state `s`. This function returns a random action $a\\in\\mathcal{A}$.\n",
    "\n",
    "> The `myrandstep(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int, a::Int)` function takes a `MyMDPProblemModel` instance, a `MyRectangularGridWorldModel` instance, the state `s` and an action `a` and returns the next state $s^{\\prime}$ and reward $r$.\n",
    "\n",
    "> The `myrollout(problem::MyMDPProblemModel, world::MyRectangularGridWorldModel, s::Int64, depth::Int64) -> Float64` function takes a `MyMDPProblemModel` instance, a `MyRectangularGridWorldModel` instance, the state `s` and the depth `d`. This function returns the cumulative reward after exploring the network for `d` steps.\n",
    "\n",
    "These implementations were based on `Algorithm 9.1` of the [Decisions Book](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc2fc656-2df9-467a-8617-a5e293c1519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrandpolicy(problem::MyMDPProblemModel, \n",
    "        world::MyRectangularGridWorldModel, s::Int)::Int\n",
    "    \n",
    "    # initialize -\n",
    "    d = Categorical([0.25,0.25,0.25,0.25]); # you specify this LRDU\n",
    "    \n",
    "    # should keep choosing -\n",
    "    should_choose_again = true;\n",
    "    a = 1; # default\n",
    "    while (should_choose_again == true)\n",
    "       \n",
    "        # initialize a random categorical distribution over actions -\n",
    "        aᵢ = rand(d);\n",
    "        \n",
    "        # get the move and the current location -\n",
    "        Δ = world.moves[aᵢ];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            a = aᵢ\n",
    "            should_choose_again = false;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return a;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "244b7a1c-ade9-47d6-a166-91fafa73cb51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrandstep(problem::MyMDPProblemModel, \n",
    "        world::MyRectangularGridWorldModel, s::Int, a::Int)\n",
    "    \n",
    "    r = problem.R[s,a]; # get the reward value for being in state s, and taking action a\n",
    "    Δ = world.moves[a]; # this action does this move    \n",
    "    current_position = world.coordinates[s]; # get where we are now\n",
    "    new_position =  current_position .+ Δ; # propose a new position\n",
    "    \n",
    "    s′ = s; # default, we don't do anything, stay where you are\n",
    "    if (haskey(world.states, new_position) == true)\n",
    "        s′ = world.states[new_position];\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    return (s′,r) # This returns the next state and the reward at the current position\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35fccd61-8532-4bfb-a345-e7dedaabc894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function myrollout(problem::MyMDPProblemModel, \n",
    "        world::MyRectangularGridWorldModel, s::Int64, depth::Int64)::Float64\n",
    "    \n",
    "    # initialize -\n",
    "    ret = 0.0;\n",
    "    γ = problem.γ;\n",
    "    visited_states = Set{Int64}();\n",
    "    is_ok_to_stop = false;\n",
    "    i = 1;\n",
    "\n",
    "    while (is_ok_to_stop == false)\n",
    "       \n",
    "        a = myrandpolicy(problem, world, s);\n",
    "        s, r = myrandstep(problem, world, s, a);\n",
    "\n",
    "        if (s ∉ visited_states)\n",
    "            push!(visited_states, s);\n",
    "            ret += r*γ^(i-1);\n",
    "            i += 1;\n",
    "\n",
    "            # can we stop?\n",
    "            if (length(visited_states) ≥ depth)\n",
    "                is_ok_to_stop = true;\n",
    "            end\n",
    "        end\n",
    "    end    \n",
    "    \n",
    "    # for i ∈ 1:depth\n",
    "    #     a = myrandpolicy(problem, world, s);\n",
    "    #     s, r = myrandstep(problem, world, s, a);\n",
    "    #     ret += r*γ^(i-1);\n",
    "    # end\n",
    "    return ret;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98e12c6b-3e73-4e98-8d91-ddbf74156877",
   "metadata": {},
   "outputs": [],
   "source": [
    "U(s) = myrollout(m,world,s,d);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e440142-f6b9-465c-9602-1550ed90a80f",
   "metadata": {},
   "source": [
    "We use a `for loop` to compute the value (utility) starting from each state in the system. For each state $s\\in\\mathcal{S}$, we call the `myrollout(...)` function, which explores the system to a depth `d`, returns the value (utility) at state `s,` and saves the value in the `utility_array::Array{Float64,1}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f8c8b82-6e21-4c94-a3ab-8cf17f77fedc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `m` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `m` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] U(s::Int64)",
      "   @ Main ./In[26]:1",
      " [2] top-level scope",
      "   @ ./In[28]:3"
     ]
    }
   ],
   "source": [
    "utility_array = Array{Float64,1}();\n",
    "for s ∈ 𝒮\n",
    "    push!(utility_array, U(s))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed50bb-90c3-479c-89b7-11a7d8dced1c",
   "metadata": {},
   "source": [
    "Extract the `action-value function` or $Q(s, a)$ from the `utility_array`. We can do this using the `Q(...)` function, which takes `m` and the `utility_array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bb39c49-cf77-422d-8f82-68b4d0978744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `m` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `m` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[30]:1"
     ]
    }
   ],
   "source": [
    "my_Q = Q(m, utility_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46295cc8-cdc8-4c23-833a-3d1152480083",
   "metadata": {},
   "source": [
    "Finally, we can extract the policy $\\pi(s)$ from the action-value function $Q(s,a)$ using the `policy(...)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "125cd5d1-c69e-4927-a0a3-46dcdf358211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `my_Q` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_Q` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[32]:1"
     ]
    }
   ],
   "source": [
    "my_π = policy(my_Q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f878bfe2-3508-42d7-a6a0-ddd91cc8f2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `my_π` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_π` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      ""
     ]
    }
   ],
   "source": [
    "my_π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed09cdd1-ef46-4be5-bf46-43bfd936a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a good policy, just in case we can't find one later -\n",
    "# save(\"Good-policy.jld2\",Dict(\"policy\"=>my_π))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939025-1913-42e6-aa71-199c954588a1",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "`Unhide` the code block below to see how we plot the path from a `startstate` to (potentially) one of the absorbing states. The charging station is show in green, while the lava pits are shown in red. Specify the `startstate` coordinate tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a697be5c-7c95-4550-9ead-4d826771381a",
   "metadata": {},
   "outputs": [],
   "source": [
    " startstate = (10,7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "495bf226-94d6-4376-a736-a650fa86db52",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `my_π` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `my_π` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[37]:18"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # setup \n",
    "    world_model = world;\n",
    "   \n",
    "    # draw the path -\n",
    "    p = plot();\n",
    "    initial_site = startstate\n",
    "    hit_absorbing_state = false\n",
    "    s = world_model.states[initial_site];\n",
    "    visited_sites = Set{Tuple{Int,Int}}();\n",
    "    push!(visited_sites, initial_site);\n",
    "\n",
    "    s′ = s;\n",
    "    while (hit_absorbing_state == false)\n",
    "        \n",
    "        current_position = world_model.coordinates[s′]\n",
    "        a = my_π[s′];\n",
    "        Δ = world_model.moves[a];\n",
    "        new_position =  current_position .+ Δ\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "        plot!([current_position[1], new_position[1]],[current_position[2],new_position[2]], label=\"\", arrow=true, lw=1, c=:gray)\n",
    "\n",
    "        s′ = nothing;\n",
    "        if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "            hit_absorbing_state = true;\n",
    "        elseif (haskey(world_model.states, new_position) == true)\n",
    "            s′ = world_model.states[new_position];\n",
    "            push!(visited_sites, new_position);\n",
    "        else\n",
    "            hit_absorbing_state = true; \n",
    "        end\n",
    "    end\n",
    "\n",
    "    # draw the grid -\n",
    "    for s ∈ 𝒮\n",
    "        current_position = world_model.coordinates[s]\n",
    "        a = my_π[s];\n",
    "        Δ = world_model.moves[a];\n",
    "        new_position =  current_position .+ Δ\n",
    "        \n",
    "         if (haskey(rewards, current_position) == true && rewards[current_position] == charging_reward)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:green, ms=4)\n",
    "        elseif (haskey(rewards, current_position) == true && rewards[current_position] == lava_reward)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:red, ms=4)\n",
    "        elseif (in(current_position, soft_wall_set) == true)\n",
    "            scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "        else\n",
    "            if (is_reward_shaping_on == true)\n",
    "                new_color = weighted_color_mean(rbf(current_position, charging_station_coordinates, σ = σ), colorant\"green\", colorant\"white\")\n",
    "                scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=new_color)\n",
    "            else\n",
    "                scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:white)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    current()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca3da4a7-0e26-4725-b380-58542bfe1678",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# let\n",
    "\n",
    "#     move_arrows = Dict{Int,Any}();\n",
    "#     move_arrows[1] = \"←\"\n",
    "#     move_arrows[2] = \"→\"\n",
    "#     move_arrows[3] = \"↓\"\n",
    "#     move_arrows[4] = \"↑\"\n",
    "#     move_arrows[5] = \"∅\";\n",
    "\n",
    "#     for s ∈ 𝒮\n",
    "#         a = my_π[s];\n",
    "#         Δ = world.moves[a];\n",
    "#         current_position = world.coordinates[s]\n",
    "#         new_position =  current_position .+ Δ\n",
    "        \n",
    "#         if (in(current_position, absorbing_state_set) == true)\n",
    "#             println(\"$(current_position) $(move_arrows[5])\")\n",
    "#         else\n",
    "#             println(\"$(current_position) $(move_arrows[a]) $(new_position)\")\n",
    "#         end\n",
    "#     end\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
