{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c7da3d-5bac-4d7d-81c2-f135ae11d8cc",
   "metadata": {},
   "source": [
    "## Example: Markov Decision Processes, Value and Policy Iteration\n",
    "\n",
    "### Markov Decision Processes (MDPs)\n",
    "In an MDP, an agent receives a reward or penalty for each decision. MDPs consist of the tuple $\\left(\\mathcal{S}, \\mathcal{A}, R_{a}\\left(s, s^{\\prime}\\right), T_{a}\\left(s,s^{\\prime}\\right), \\gamma\\right)$:\n",
    "\n",
    "* The state space $\\mathcal{S}$ is the set of all possible states $s$ that a system can exist in.\n",
    "* The action space $\\mathcal{A}$ is the set of all possible actions $a$ that are available to the agent, where $\\mathcal{A}_{s} \\subseteq \\mathcal{A}$ is the subset of the action space $\\mathcal{A}$ that is accessible from state $s$.\n",
    "* An reward $R_{a}\\left(s, s^{\\prime}\\right)$ is received after transitioning from $s\\rightarrow{s}^{\\prime}$ due to action $a$. \n",
    "* The state transition model $T_{a}\\left(s,s^{\\prime}\\right) = P(s_{t+1} = s^{\\prime}~|~s_{t}=s,a_{t} = a)$ denotes the probability that action $a$ in state $s$ at time $t$ will result in state $s^{\\prime}$ at time $t+1$\n",
    "* The quantity $\\gamma$ is a discount factor used to weigh the future expected utility.\n",
    "\n",
    "Finally, a policy function $\\pi$ is the mapping from states $s\\in\\mathcal{S}$ to actions $a\\in\\mathcal{A}$ used by the agent to solve a decision task, i.e., $\\pi(s) = a$.\n",
    "\n",
    "### Value Iteration\n",
    "_Value iteration_ iteratively computes the optimal value function $U^{*}(s)$ using the _Bellman backup_ operation:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "U_{k+1}(s) = \\underset{a\\in\\mathcal{A}}{\\max}\\left(R(s,a) + \\gamma\\cdot\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime}\\,\\vert\\,s,a)\\cdot{U}_{k}(s^{\\prime})\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "As $k\\rightarrow\\infty$ the value function $U_{k}(s)$ converges to the optimal value function $U^{\\star}(s)$. The optimal policy $\\pi^{\\star}(s)$ can be extracted from the $Q(s,a)$-function:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Q^{\\star}(s,a) = R(s,a) + \\gamma\\times{\\text{sum}([T(s,s^{\\prime},a)\\times{U^{\\star}}(s^{\\prime})\\,\\,\\text{for}\\,s^{\\prime} \\in\\mathcal{S}])}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "by selecting the action $a$ such that:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi^{\\star}(s) = \\underset{a\\in\\mathcal{A}}{\\arg\\max}\\,Q^{\\star}(s,a)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "### Learning objectives\n",
    "Suppose you have a [roomba](https://www.irobot.com) that has finished cleaning the floor in your kitchen and needs to return to its charging station. However, between your kitchen and the `charging station` (home base and safety), there are one or more `lava pits` (destruction for the [roomba](https://www.irobot.com)). This is an example of a two-dimensional grid-world navigational decision task. \n",
    "\n",
    "`Lab 9c` will familiarize students with using `value iteration` for solving a two-dimensional grid-world navigation task. In particular, we will:\n",
    "\n",
    "* __Task 1__: Setup a $n_{r}\\times{n}_{c}$ grid, encoded this model as an instance of the `MyRectangularGridWorldModel` type\n",
    "    * `TODO`: Inspect the data inside our grid world model, understand what each describes\n",
    "* __Task 2__: Use our `MyRectangularGridWorldModel` instance and generate the components of the `MDP`, namely, the return function (or array) `R(s, a)`, and the model of the physics of the world in the transition function (or array) `T(s, s‚Ä≤, a)`.\n",
    "* __Task 3__: Use a `value iteration` method to estimate the optimal value function $U^{\\star}(s)$\n",
    "    * `TODO`: Extract the `action-value function` or $Q(s, a)$ from the optimal optimal value function $U^{\\star}(s)$ \n",
    "    * `TODO`: Compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$\n",
    "    * `TODO`: Visualize the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91c6d-e13e-4b8c-bf14-d4651ef03d5e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The computations in this lab (or example) are enabled by the [VLDecisionsPackage.jl](https://github.com/varnerlab/VLDecisionsPackage.jl.git) and several external `Julia` packages. To load the required packages and any custom codes the teaching team has developed to work with these packages, we [include](https://docs.julialang.org/en/v1/manual/code-loading/) the `Include.jl` file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6de82-32d0-4ea2-93b5-6c55c94d2e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-Fall-2024/lecture/week-12/L12a/Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90108b-9ca9-4ab9-aba1-5085799c43fd",
   "metadata": {},
   "source": [
    "## Task 1: Build the world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's setup the data for the world, setup the states, actions, rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` that are avialble to the agent and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states, and setup the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d2229-9484-4320-ad55-3c597a8b3881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 5\n",
    "number_of_cols = 5\n",
    "nactions = 4;\n",
    "Œ≥ = 0.95;\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "ùíÆ = range(1,stop=nstates,step=1) |> collect;\n",
    "ùíú = range(1,stop=nactions,step=1) |> collect;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714e902-b72d-479d-bac6-537c7fa33649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ùíú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea7b15-294b-485a-8b59-3442fc339f06",
   "metadata": {},
   "source": [
    "Next, we'll set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. We only need to put `non-default` reward values in the reward dictionary (we'll add default values to the other locations later). Lastly, let's put the locations on the grid that are `absorbing`, meaning the charging station or lava pits in your living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ea9b1-a9f9-4628-9bb7-f43d7811f970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup rewards -\n",
    "lava_reward = -1000.0;\n",
    "charging_reward = 100.0\n",
    "\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(1,2)] = lava_reward # lava in the (1,2) square \n",
    "rewards[(2,2)] = lava_reward # lava in the (2,2) square \n",
    "rewards[(2,3)] = lava_reward # lava in the (2,3) square \n",
    "rewards[(4,5)] = lava_reward # lava in the (4,4) square\n",
    "rewards[(4,3)] = charging_reward   # charging station square\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "for (k,v) ‚àà rewards\n",
    "    push!(absorbing_state_set, k);   \n",
    "end\n",
    "\n",
    "# setup soft walls (constraints) -\n",
    "soft_wall_set = Set{Tuple{Int,Int}}();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf87c2-0b91-458e-8fb0-bb0db4e451ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we can build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* We must pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f48d7a-a171-418d-8e99-cedb894ef305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af1d83-8feb-47ff-b100-1bc01bfe7ef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Questions\n",
    "__TODO__: Inspect the data inside our grid world model and understand what each field describes. Let's look at: `coordinates, states, moves` and `rewards.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fbd99-9eb9-4c8c-a245-e6bc7151b8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world.states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed737-187b-4c88-88e4-dcca9d79e66a",
   "metadata": {},
   "source": [
    "## Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s‚Ä≤, a)`. Let's construct these from our grid world model instance, starting with the reward function `R(s, a)`:\n",
    "\n",
    "### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Œî`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51634b4-76cf-4533-81d2-60e1470117dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ‚àà ùíÆ\n",
    "    for a ‚àà ùíú\n",
    "        \n",
    "        Œî = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            if (haskey(rewards, new_position) == true)\n",
    "                R[s,a] = rewards[new_position];\n",
    "            else\n",
    "                R[s,a] = -1.0;\n",
    "            end\n",
    "        else\n",
    "            R[s,a] = -50000.0; # we are off the grid, big negative penalty\n",
    "        end\n",
    "    end\n",
    "end\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40727829-a6d9-446a-8384-83828d967028",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Œî`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s‚Ä≤,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d92d6f-fcd5-44be-acc3-58a275d2987e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ‚àà ùíú\n",
    "    \n",
    "    Œî = world.moves[a];\n",
    "    \n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s‚Ä≤ = world.states[new_position];\n",
    "            T[s, s‚Ä≤,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c291884-62b4-4b4b-8646-ff5fb0b7447a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(T[24,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f983f3d-753a-4e5a-92f5-52f941a4d53a",
   "metadata": {},
   "source": [
    "Finally, we construct an instance of the `MyMDPProblemModel` which encodes the data required to solve the MDP problem.\n",
    "* We must pass the states `ùíÆ`, the actions `ùíú`, the transition matrix `T`, the reward matrix `R`, and the discount factor `Œ≥` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79abec-c27e-49ac-acf4-be56211c3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = build(MyMDPProblemModel, (ùíÆ = ùíÆ, ùíú = ùíú, T = T, R = R, Œ≥ = Œ≥));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d40706-61bf-4927-8b0a-8fa54a829c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.ùíÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4044aa-b380-4a8c-9252-eae71914e1e3",
   "metadata": {},
   "source": [
    "## Task 3: Estimate the optimal value function $U^{\\star}(s)$\n",
    "Let's explore value iteration by first constructing an instance of the `MyValueIterationModel` type, which takes the maximum number of iterations as a parameter. Save this in the `value_iteration_model` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a8314-d55e-4f0c-981b-f7a406843ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_iteration_model = MyValueIterationModel(1000); # what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1629357-940e-4570-a267-8f978426e82a",
   "metadata": {},
   "source": [
    "Next, we call the `solve(...)` method by passing the `value_iteration_model` instance and our MDP model `m::MyMDPProblemModel` as arguments. The `solve(...)` method iteratively computes the value function $U^{\\star}(s)$, by calling the `backup(...)` function, which in turn calls the `lookahead(...)` function:\n",
    "* The `solve(...)` method iteratively computes the optimal value function $U^{\\star}(s)$ and returns it in an instance of the `MyValueFunctionPolicy` type. The policy is stored in the `U` field.\n",
    "* __Note__: An alternative to value iteration to estimate the utility at each position would be the `rollout(...)` method that we used in the discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b5bc5-d9a3-48c1-8eac-13cb595feea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution = solve(value_iteration_model, m);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3eca1a-d9b8-4a10-b7bf-fde455b51347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "solution.U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f318f4-c389-49b5-8d46-08e1df83e58f",
   "metadata": {},
   "source": [
    "### Questions\n",
    "`TODO`: Extract the `action-value function` or $Q(s, a)$ from the optimal optimal value function $U^{\\star}(s)$. We can do this using the `Q(...)` function, which takes `m::MyMDPProblemModel` and the `solution::MyValueFunctionPolicy`\n",
    "    \n",
    "```julia\n",
    "function Q(p::MyMDPProblemModel, U::Array{Float64,1})::Array{Float64,2}\n",
    "\n",
    "    # grab stuff from the problem\n",
    "    ùíÆ, T, R, Œ≥ = p.ùíÆ, p.T, p.R, p.Œ≥;\n",
    "\n",
    "    # initialize -\n",
    "    Q_array = Array{Float64,2}(undef, length(ùíÆ), length(ùíú))\n",
    "\n",
    "    # main loop\n",
    "    for s ‚àà 1:length(ùíÆ)\n",
    "        for a ‚àà 1:length(ùíú)\n",
    "            Q_array[s,a] = R[s,a] + Œ≥*sum([T[s, s‚Ä≤,a]*U[s‚Ä≤] for s‚Ä≤ in ùíÆ]);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return Q_array\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e1e12-f637-4ecb-af32-cd0b3fd56fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_Q = Q(m, solution.U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc0626-8041-40e9-b92d-491b6edfe9dd",
   "metadata": {},
   "source": [
    "`TODO`: Compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$. We can do this using the `policy(...)` function:\n",
    " ```julia\n",
    " function policy(Q_array::Array{Float64,2})::Array{Int64,1}\n",
    "\n",
    "    # get the dimension -\n",
    "    (NR, _) = size(Q_array);\n",
    "\n",
    "    # initialize some storage -\n",
    "    œÄ_array = Array{Int64,1}(undef, NR)\n",
    "    for s ‚àà 1:NR\n",
    "        œÄ_array[s] = argmax(Q_array[s,:]);\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    return œÄ_array;\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ba794-1995-48d1-95d5-5aceefbafbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_œÄ = policy(my_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fcde3-b949-4469-ad96-d51f978a28a8",
   "metadata": {},
   "source": [
    "`TODO`: Visualize the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433b845-d226-4501-9d19-ea4b34efb1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "move_arrows = Dict{Int,Any}();\n",
    "move_arrows[1] = \"‚Üê\"\n",
    "move_arrows[2] = \"‚Üí\"\n",
    "move_arrows[3] = \"‚Üì\"\n",
    "move_arrows[4] = \"‚Üë\"\n",
    "move_arrows[5] = \"‚àÖ\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4ef98-b525-49ca-a414-826122781e4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for s ‚àà ùíÆ\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    new_position =  current_position .+ Œî\n",
    "    \n",
    "    if (in(current_position, absorbing_state_set) == true)\n",
    "        println(\"$(current_position) $(move_arrows[5])\")\n",
    "    else\n",
    "        println(\"$(current_position) $(move_arrows[a]) $(new_position)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a97547-498c-4d8a-be5c-e5d92df5d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ùíÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a90487-38cc-43e3-9292-f3e651784878",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# draw the path -\n",
    "p = plot();\n",
    "s = (1,1);\n",
    "\n",
    "visited_sites = Set{Tuple{Int,Int}}();\n",
    "# push!(visited_sites, initial_site);\n",
    "\n",
    "while (in(s, absorbing_state_set) == false)\n",
    "\n",
    "    state = world.states[s];\n",
    "    a = my_œÄ[state];\n",
    "    Œî = world.moves[a];\n",
    "    s_next =  s .+ Œî\n",
    "\n",
    "    @show s, s_next\n",
    "\n",
    "    scatter!([s[1]],[s[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "    plot!([s[1], s_next[1]],[s[2], s_next[2]], label=\"\", arrow=true, lw=1, c=:gray)\n",
    "    \n",
    "    if (haskey(world.states, s_next) == true)\n",
    "        push!(visited_sites, s_next);\n",
    "        s = s_next;\n",
    "    end\n",
    "end\n",
    "\n",
    "# draw the grid -\n",
    "for s ‚àà ùíÆ\n",
    "    \n",
    "    current_position = world.coordinates[s]\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world.moves[a];\n",
    "    new_position =  current_position .+ Œî\n",
    "    \n",
    "    if (haskey(rewards, current_position) == true && rewards[current_position] == charging_reward)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:green, ms=6)\n",
    "    elseif (haskey(rewards, current_position) == true && rewards[current_position] == lava_reward)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:red, ms=6)\n",
    "    else (in(current_position, soft_wall_set) == true)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=4)\n",
    "    end\n",
    "end\n",
    "current()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
