{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cde366-e417-4e53-bfc8-20a18fcf23ad",
   "metadata": {},
   "source": [
    "## Example: The Tiger Problem as a Markov Decision Problem (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b6c77-55bd-434c-9f74-8dc59a75631f",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"figs/Fig-Linear-MDP-Schematic.png\" style=\"align:right; width:80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1051985-7e86-4c66-bb83-994e1c0ad0d6",
   "metadata": {},
   "source": [
    "An agent trapped in a long hallway with two doors at either end. Behind the red door is a tiger (and certain death), while behind the green door is freedom. If the agent opens the red door, the agent is eaten (and receives a large negative reward). However, if the agent opens the green door, it escapes and gets a positive reward. \n",
    "\n",
    "For this problem, the MDP has the tuple components:\n",
    "* $\\mathcal{S} = \\left\\{1,2,\\dots,N\\right\\}$ while the action set is $\\mathcal{A} = \\left\\{a_{1},a_{2}\\right\\}$; action $a_{1}$ moves the agent one state to the right, action $a_{2}$ moves the agent one state to the left.\n",
    "* The agent receives a reward of +10 for entering state 1 (escapes). However, the agent is penalized -100 for entering state N (eaten by the tiger).  Finally, the agent is not charged to move to adjacent locations.\n",
    "* Let the probability of correctly executing the action $a_{j}$ be $\\alpha$\n",
    "\n",
    "Let's compute $U^{\\pi}(s)$ for different choices for the policy function $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dbaee-739d-4e02-a3e2-8721fb8c636f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's load some packages that are required for the example by calling the `include(...)` function on our initialization file `Include.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6aa8155-d399-4482-904f-c3edaa65244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-13/L13c`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-13/L13c/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-13/L13c/Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-13/L13c/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-13/L13c/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074882e6-d1d5-461b-81ac-2de90d471721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some global constants -\n",
    "Î± = 0.75; # probability of moving the direction we are expect\n",
    "Î³ = 0.95; # discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b31d94-e11c-4fd9-9871-d22104f22a88",
   "metadata": {},
   "source": [
    "## Setup the states, actions, transitions and rewards\n",
    "Let's set up the states $\\mathcal{S}$ and actions $\\mathcal{A}$ sets: \n",
    "* We have `safety` at index `1`, but the `tiger` lives at index `10`. Thus, the states set $\\mathcal{S} = \\left\\{1,\\dotsc,10\\right\\}$\n",
    "* We have `3` possible actions, `move left,` `move right` or `stand still` in action set $\\mathcal{A} = \\left\\{1,2,3\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a687bf-4977-4346-8665-8de40248baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the states and actions -\n",
    "safety = 1;\n",
    "tiger = 10;\n",
    "\n",
    "# Setup the states -\n",
    "states = range(safety,stop=tiger, step=1) |> collect;\n",
    "\n",
    "# Setup the actions\n",
    "actions = [1,2,3]; # aâ‚ = move left, aâ‚‚ = move right, aâ‚ƒ = stand still"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118e9ab-4dd3-4f17-8421-ac56a4cdc710",
   "metadata": {},
   "source": [
    "### Setup the rewards array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11484eb5-4b7b-4c48-9e29-8324f6504569",
   "metadata": {},
   "source": [
    "The rewards $\\mathbf{R}$ is a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array whose $R(s,a)$ element holds the reward for performing action $a$ in state $s$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988a7ae7-5352-480b-aa9a-4286a9b279df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the rewards -\n",
    "R = Array{Float64,2}(undef,length(states), length(actions));\n",
    "fill!(R,0.0) # fill R 0.0\n",
    "R[safety + 1, 1] = 10; # reward for entering escape door\n",
    "R[tiger - 1, 2] = -100; # reward for opening door with the tiger \n",
    "R[2:end-1, 3] .= -1; # reward for doing doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ef0b8e-336b-4240-b745-d6d231f2b60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10Ã—3 Matrix{Float64}:\n",
       "  0.0     0.0   0.0\n",
       " 10.0     0.0  -1.0\n",
       "  0.0     0.0  -1.0\n",
       "  0.0     0.0  -1.0\n",
       "  0.0     0.0  -1.0\n",
       "  0.0     0.0  -1.0\n",
       "  0.0     0.0  -1.0\n",
       "  0.0     0.0  -1.0\n",
       "  0.0  -100.0  -1.0\n",
       "  0.0     0.0   0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d0171-216f-4a53-a2ac-e11f30605ce8",
   "metadata": {},
   "source": [
    "### Setup the transitions array\n",
    "The transition probability array $\\mathbf{T}(s^{\\prime}\\,|\\,s,a)$ is a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multi-dimensional array](https://docs.julialang.org/en/v1/manual/arrays/#man-multi-dim-arrays) that encodes the physics of the world, and what happens if we take action $a\\in\\mathcal{A}$ \n",
    "* For an action $a\\in\\mathcal{A}$, the array $\\mathbf{T}(s^{\\prime}\\,|\\,s,a)$ (which we sometimes denote as $\\mathbf{T}_{a}$) is a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}$ array encoding states $s$ on the rows, and states $s^{\\prime}$ on the columns. Because the entries are probabilities, the rows of $\\mathbf{T}_{a}$ must sum to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35205b5f-517c-4bd4-be65-7fe9daad3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the transitions\n",
    "T = Array{Float64,3}(undef, length(states), length(states), length(actions));\n",
    "fill!(T,0.0);\n",
    "\n",
    "# We need to put values into the transition array (these are probabilities, so eah row much sum to 1)\n",
    "T[safety, 1, 1:length(actions)] .= 1.0; # if we are in state 1, we stay in state 1 âˆ€a âˆˆ ð’œ\n",
    "T[tiger, tiger, 1:length(actions)] .= 1.0; # if we are in state 5, we stay in state 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bf93f-8f45-4657-84f4-efd81e0a7a7b",
   "metadata": {},
   "source": [
    "#### Left, Right, and Listen to Actions\n",
    "We encode the probability of reaching the next state $s^{\\prime}\\leftarrow(s, a)$ in the entries of $\\mathbf{T}_{a}$. In response to action $a\\in\\mathcal{A}$, there is a probability of $\\alpha$ that we end up in the correct next state, but a $(1-\\alpha)$ that we end up in an incorrect state. \n",
    "\n",
    "`Unhide` the block below to see how we populate the transition probability array $\\mathbf{T}(s^{\\prime}\\,|\\,s,a)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ab3482-3840-4156-b51f-8d32ca7b9e6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# left actions -\n",
    "for s âˆˆ 2:(tiger - 1)\n",
    "    T[s,s-1,1] = Î±;\n",
    "    T[s,s+1,1] = (1-Î±);\n",
    "end\n",
    "\n",
    "# right actions -\n",
    "for s âˆˆ 2:(tiger - 1)\n",
    "    T[s,s-1,2] = (1-Î±);\n",
    "    T[s,s+1,2] = Î±; \n",
    "end\n",
    "\n",
    "# listen action (we don't move to a new state)\n",
    "for s âˆˆ 2:(tiger-1)\n",
    "    T[s,s,3] = 1.0;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d064507-f9da-406d-9275-6455e22fe899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10Ã—10 Matrix{Float64}:\n",
       " 1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " 0.75  0.0   0.25  0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       " 0.0   0.75  0.0   0.25  0.0   0.0   0.0   0.0   0.0   0.0\n",
       " 0.0   0.0   0.75  0.0   0.25  0.0   0.0   0.0   0.0   0.0\n",
       " 0.0   0.0   0.0   0.75  0.0   0.25  0.0   0.0   0.0   0.0\n",
       " 0.0   0.0   0.0   0.0   0.75  0.0   0.25  0.0   0.0   0.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.75  0.0   0.25  0.0   0.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.75  0.0   0.25  0.0\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.75  0.0   0.25\n",
       " 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[:,:,1] # probability matrix for taking action aáµ¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7cfca-fe0e-4d4c-aefc-d53a4f4fd745",
   "metadata": {},
   "source": [
    "## Build the MDP problem object and estimate the utility $U^{\\pi}$ \n",
    "Construct a `MyMDPProblemModel` instance with values associated with the problem. \n",
    "* We must pass the states `ð’®,` the actions `ð’œ,` the transition matrix `T,` the reward matrix `R,` and the discount factor `Î³` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bfa4d95-a158-41c7-b178-a71b7cc54c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build(MyMDPProblemModel, \n",
    "    (ð’® = states, ð’œ = actions, T = T, R = R, Î³ = Î³));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa9f26-c14e-4d15-b98b-a6d1d7845e3f",
   "metadata": {},
   "source": [
    "Next, let's formulate some simple policies for this problem, `always_move_right,` `always_move_left`, and `always_listen.` We'll test how good or bad these policies are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4376ec9-db21-4b15-ac68-14c00662707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "always_move_right(s) = 2;\n",
    "always_move_left(s) = 1;\n",
    "always_listen(s) = 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0aa78d-c1f6-4fd3-826f-c5b6055452e2",
   "metadata": {},
   "source": [
    "## Policy evaluation\n",
    "We compute a policyâ€™s value function (utility) using the `iterative_policy_evaluation` function, which evaluates the _immediate benefit_ of implementing the policy $\\pi(s)$ and the _future benefit_ of looking ahead over states. \n",
    "\n",
    "* The `iterative_policy_evaluation(...)` function improves our estimate of the value (utility) of a policy $U^{\\pi}(s)$ by iteration. The `iterative_policy_evaluation` function takes a `MyMDPProblemModel` instance, a policy function, and the maximum number of iterations to refine the value estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0da09b-308f-4aec-a06d-d584f1ea1673",
   "metadata": {},
   "source": [
    "```julia\n",
    "function iterative_policy_evaluation(p::MyMDPProblemModel, policy::Function, k_max::Int)\n",
    "\n",
    "    # grab stuff from the problem -\n",
    "    ð’®, T, R, Î³ = p.ð’®, p.T, p.R, p.Î³;\n",
    "\n",
    "    # initialize U vector\n",
    "    U = [0.0 for s âˆˆ ð’®];\n",
    "\n",
    "    # solve -\n",
    "    for _ âˆˆ 1:k_max\n",
    "        U = [lookahead(p, U, s, policy(s)) for s âˆˆ ð’®]\n",
    "    end\n",
    "\n",
    "    # return U -\n",
    "    return U;\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0feba0-f8b0-4c75-b839-f01990056ab2",
   "metadata": {},
   "source": [
    "The `lookahead(...)` function computes the immediate and future benefit. The `lookahead` function takes the `MyMDPProblemModel` instance, the value vector `U,` the state `s,` and the action `a` as arguments and returns the total value of the policy (immediate plus future benefit):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279bfc3c-af3f-4bff-8fa7-b8264adbcf6d",
   "metadata": {},
   "source": [
    "```julia\n",
    "function lookahead(p::MyMDPProblemModel, U::Vector{Float64}, s::Int64, a::Int64)\n",
    "    \n",
    "    # get data from the problem\n",
    "    ð’®, T, R, Î³ = p.ð’®, p.T, p.R, p.Î³;\n",
    "    \n",
    "    # compute -\n",
    "    return R[s,a] + Î³*sum(T[s, sâ€²,a]*U[i] for (i,sâ€²) in enumerate(ð’®))\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c66f86-93e4-4a70-9e04-dfde6e449383",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPE = iterative_policy_evaluation(m, always_move_right, 50*length(states));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50e94765-00a9-41d1-a309-19fb0ba0a06f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       "    0.0\n",
       "  -47.23239230918141\n",
       "  -66.29107692516689\n",
       "  -77.2959771954194\n",
       "  -86.3885563869365\n",
       "  -95.48177095161319\n",
       " -105.21331762767126\n",
       " -115.8405572304044\n",
       " -127.51213234222104\n",
       "    0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca09a4-642f-4b37-939d-91aa26c1b630",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Estimate the Action-value or Q-function\n",
    "The quantity being computed in the `lookahead(...)` function is called the _action-value_ or _Q-function_ $Q(s, a)$. From $Q(s,a)$ we can compute the value function $U(s)$ by selecting the action $a$ that maximizes the _Q-function_:\n",
    "\\begin{equation*}\n",
    "U(s) = \\underset{a\\in\\mathcal{A}}{\\max}\\,Q(s,a)\n",
    "\\end{equation*}\n",
    "and the policy $\\pi(s)$:\n",
    "\\begin{equation*}\n",
    "\\pi(s) = \\underset{a\\in\\mathcal{A}}{\\arg\\max}\\,Q(s,a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c049d20-7c42-4999-8f08-386feb86d40a",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "* We compute the `Q-function` by calling the `Q(...)` function:\n",
    "\n",
    "```julia\n",
    "function Q(p::MyMDPProblemModel, U::Array{Float64,1})::Array{Float64,2}\n",
    "\n",
    "    # grab stuff from the problem\n",
    "    ð’®, T, R, Î³ = p.ð’®, p.T, p.R, p.Î³;\n",
    "\n",
    "    # initialize -\n",
    "    Q_array = Array{Float64,2}(undef, length(ð’®), length(ð’œ))\n",
    "\n",
    "    for s âˆˆ 1:length(ð’®)\n",
    "        for a âˆˆ 1:length(ð’œ)\n",
    "            Q_array[s,a] = R[s,a] + Î³*sum([T[s, sâ€²,a]*U[sâ€²] for sâ€² in ð’®]);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return Q_array\n",
    "end\n",
    "```\n",
    "\n",
    "* We estimate the best policy using the `policy(...)` function:\n",
    "\n",
    "```julia\n",
    "function policy(Q_array::Array{Float64,2})::Array{Int64,1}\n",
    "\n",
    "    # get the dimension -\n",
    "    (NR, _) = size(Q_array);\n",
    "\n",
    "    # initialize some storage -\n",
    "    Ï€_array = Array{Int64,1}(undef, NR)\n",
    "    for s âˆˆ 1:NR\n",
    "        Ï€_array[s] = argmax(Q_array[s,:]);\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    return Ï€_array;\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "499f1032-207a-4a4e-8578-e21e0e11fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8Ã—3 Matrix{Float64}:\n",
       " 12.7512     8.25364  11.1137\n",
       " 11.5841    10.5249   10.0049\n",
       " 10.5213     9.55429   8.99527\n",
       "  9.54818    8.654     8.07077\n",
       "  8.63886    7.77503   7.20691\n",
       "  7.7296     6.77497   6.34312\n",
       "  6.62911    5.20109   5.29765\n",
       "  4.72324  -98.4256    3.48708"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QPE_array = Q(m, UPE)[2:end-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102ae6c8-f546-4e7c-ba97-97030588d022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_policy = policy(QPE_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e44a9d-7c62-482f-92b0-6f584b274685",
   "metadata": {},
   "source": [
    "### Greedy policy\n",
    "Given a value (utility) function $U(s)$, we can construct a $\\textit{greedy}$ policy $\\pi(s)$ by selecting the action $a$ that maximizes the expected utility of the next state $s^{\\prime}$:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi(s) = \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\left(R(s,a)+\\gamma\\cdot\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime}\\,\\vert\\, s,a)\\cdot{U}(s^{\\prime})\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Let's explore this idea by constructing a greedy policy. First, we need to generate a value (utility) function $U(s)$, let's suppose it's random (uniform random between `safety` and `tiger`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aab2aff-d177-4a9e-9568-08adb6aa1ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uâ‚’ = [rand() for s âˆˆ states];\n",
    "Uâ‚’[safety] = 1000\n",
    "Uâ‚’[tiger] = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74a90724-1eb5-4bae-a536-aaa6f78a1ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       "  1000.0\n",
       "     0.981573344299635\n",
       "     0.14541689972201743\n",
       "     0.9829701423368213\n",
       "     0.8897969079855844\n",
       "     0.8467567395575449\n",
       "     0.574374688529497\n",
       "     0.38799560238170094\n",
       "     0.8899506739169001\n",
       " -1000.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uâ‚’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e115e-6a86-421e-8420-942d778e01bc",
   "metadata": {},
   "source": [
    "Next, let's create a `MyValueFunctionPolicy` instance, that takes the problem variable `m` and our random value vector $U_{\\circ}(s)$ as arguments in its constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d12113-bbcd-42fa-9327-3ad98cfc7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_greedy_value_policy = MyValueFunctionPolicy(m, Uâ‚’);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8c2061-4ed3-4776-a889-33d3d80fcbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10Ã—2 Matrix{Float64}:\n",
       "  1.0   1000.0\n",
       "  2.0      0.981573\n",
       "  3.0      0.145417\n",
       "  4.0      0.98297\n",
       "  5.0      0.889797\n",
       "  6.0      0.846757\n",
       "  7.0      0.574375\n",
       "  8.0      0.387996\n",
       "  9.0      0.889951\n",
       " 10.0  -1000.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1:10 my_greedy_value_policy.U]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c30eea-7a7f-4cc2-8134-534119db853d",
   "metadata": {},
   "source": [
    "Finally, we can estimate a policy using the `greedy(...)` function in combination with `a very Julia` way of calling the `greedy(...)` function:\n",
    "\n",
    "```julia\n",
    "function greedy(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64)\n",
    "    u, a = findmax(a->lookahead(problem, U, s, a), problem.ð’œ);\n",
    "    return (a=a, u=u)\n",
    "end\n",
    "\n",
    "(Ï€::MyValueFunctionPolicy)(s::Int64) = greedy(Ï€.problem, Ï€.U, s).a;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3330e42c-2913-48e7-b5ca-b05a8014efe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_greedy_value_policy(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4209bdbb-7722-48fd-a3c9-270bdba50b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8Ã—3 Matrix{Float64}:\n",
       "  722.535      237.604     -0.0675053\n",
       "    0.932826     0.93349   -0.861854\n",
       "    0.314936     0.668517  -0.0661784\n",
       "    0.901471     0.83677   -0.154693\n",
       "    0.770394     0.620569  -0.195581\n",
       "    0.695463     0.477552  -0.454344\n",
       "    0.620605     0.770504  -0.631404\n",
       " -237.224     -812.408     -0.154547"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QGREEDY_array = Q(m,Uâ‚’)[2:end-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b971c-7b8f-4217-8c55-769897ba1e16",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "`Value iteration` iteratively computes the optimal value function $U^{*}(s)$ using the `Bellman backup` operation:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "U_{k+1}(s) = \\underset{a\\in\\mathcal{A}}{\\max}\\left(R(s,a) + \\gamma\\cdot\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime}\\,\\vert\\,s,a)\\cdot{U}_{k}(s^{\\prime})\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "As $k\\rightarrow\\infty$ the value function $U_{k}(s)$ converges to the optimal value function $U^{\\star}(s)$.The optimal policy $\\pi^{\\star}(s)$ can be extracted from the $Q(s,a)$-function (which is constructed from $U^{\\star}(s)$):\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Q^{\\star}(s,a) = R(s,a) + \\gamma\\times{\\text{sum}([T(s,s^{\\prime},a)\\times{U^{\\star}}(s^{\\prime})\\,\\,\\text{for}\\,s^{\\prime} \\in\\mathcal{S}])}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "by selecting the action $a$ such that:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi^{\\star}(s) = \\underset{a\\in\\mathcal{A}}{\\arg\\max}\\,Q^{\\star}(s,a)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "#### Implementation\n",
    "Let's explore value iteration by first constructing an instance of the `MyValueIterationModel` type, which takes the maximum number of iterations as a parameter. Save this in the `value_iteration_model` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddd5ec3e-ff1f-4e42-9226-3ef93ba244b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_iteration_model = MyValueIterationModel(2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557baa9-8d14-418b-b8f4-57814136a92e",
   "metadata": {},
   "source": [
    "Next, we call the `solve(...)` method by passing the `value_iteration_model` instance and our MDP model `m::MyMDPProblemModel` as arguments. The `solve(...)` method iteratively computes the value function $U^{\\star}(s)$, by calling the `backup(...)` function, which in turn calls the `lookahead(...)` function:\n",
    "\n",
    "```julia\n",
    "\n",
    "function backup(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64)\n",
    "    return maximum(lookahead(problem, U, s, a) for a âˆˆ problem.ð’œ);\n",
    "end\n",
    "\n",
    "function solve(model::MyValueIterationModel, problem::MyMDPProblemModel)::MyValueFunctionPolicy\n",
    "    \n",
    "    # initialize\n",
    "    k_max = model.k_max\n",
    "    U = [0.0 for _ âˆˆ problem.ð’®];\n",
    "\n",
    "    # main loop -\n",
    "    for _ âˆˆ 1:k_max\n",
    "        U = [backup(problem, U, s) for s âˆˆ problem.ð’®];\n",
    "    end\n",
    "\n",
    "    return MyValueFunctionPolicy(problem, U);\n",
    "end\n",
    "```\n",
    "\n",
    "The `solve(...)` method iteratively computes the optimal value function $U^{\\star}(s)$ and returns it in an instance of the `MyValueFunctionPolicy` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e46cea1-53f2-470a-bffe-ec9306b93bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyValueFunctionPolicy(MyMDPProblemModel([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3], [1.0 0.0 â€¦ 0.0 0.0; 0.75 0.0 â€¦ 0.0 0.0; â€¦ ; 0.0 0.0 â€¦ 0.0 0.25; 0.0 0.0 â€¦ 0.0 1.0;;; 1.0 0.0 â€¦ 0.0 0.0; 0.25 0.0 â€¦ 0.0 0.0; â€¦ ; 0.0 0.0 â€¦ 0.0 0.75; 0.0 0.0 â€¦ 0.0 1.0;;; 1.0 0.0 â€¦ 0.0 0.0; 0.0 1.0 â€¦ 0.0 0.0; â€¦ ; 0.0 0.0 â€¦ 1.0 0.0; 0.0 0.0 â€¦ 0.0 1.0], [0.0 0.0 0.0; 10.0 0.0 -1.0; â€¦ ; 0.0 -100.0 -1.0; 0.0 0.0 0.0], 0.95), [0.0, 10.0, 7.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = solve(value_iteration_model, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74ed898b-5aae-4fd2-a882-6c77397180fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       "  0.0\n",
       " 10.0\n",
       "  7.125\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5579c8a4-cad9-434c-9d0e-44514f31e3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Ï€ = Q(m, test.U)[2:end-1,:] |> policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
