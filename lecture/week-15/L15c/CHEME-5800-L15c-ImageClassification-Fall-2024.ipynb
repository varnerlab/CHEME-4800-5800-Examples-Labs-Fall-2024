{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301524c-c75f-4769-9421-b31c4fc6f0e0",
   "metadata": {},
   "source": [
    "# Example: Develop a Multiclass Artificial Neural Network Image Classifier\n",
    "This example will familiarize students with constructing, training, and testing a simple [Feed-Forward Artificial Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network) that will classify images of handwritten numbers between `0,...,9` taken from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). Each digit between `0` and `9` has approximately 5000 example images, each of which is a `28`$\\times$`28` grayscale image; thus, each image has `784` pixels.  \n",
    "\n",
    "* In this example, we'll use the [Flux.jl package](https://github.com/FluxML/Flux.jl) to build, train, and test our image classifier. However, there are two excellent libraries for ANNs in Python (sort of), namely the [PyTorch library](https://pytorch.org/) from the [AI group at META](https://ai.meta.com/meta-ai/) and the [TensorFlow library](https://www.tensorflow.org/) developed by [Google](https://research.google/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0043b-6b9a-4851-b1ff-ce4fef45e091",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This example requires several external libraries and a function to compute the outer product. Let's download and install these packages and call our `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc952ea-d4e4-449b-aec7-ddc44d741972",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f403b43-7d09-4058-a96c-aae3bbfca8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension(file::String) = file[findlast(==('.'), file)+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc914a79-bf7f-4889-b1d1-54003da268d8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before training and testing the `ANN,` we need to construct two datasets. First, we need to build a `training dataset` of images that we will use to estimate the model parameters. We'll save training data in the `training_image_dataset` variable. Next, we'll construct a `test dataset,` which we'll use to see how well our `ANN` predicts data it has never seen. We'll save this data in the `testing_image_dataset` variable.\n",
    "* Both the `training_image_dataset` and `testing_image_dataset` will be of type `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` where the first element is the input data `x.` The second element is the `label,` i.e., whether the image corresponds to `0,....,9`.\n",
    "* The `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` type has a couple of weird features. First, notice that the floating point is `Float32`, not the default `Float64`. Next, the labels are [One Hot ecoded](https://en.wikipedia.org/wiki/One-hot). Finally, the input data `x` is a Vector, not a Matrix (even though the original image is a matrix of `Gray` values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583d83-6353-450f-9a6f-2ec5b44185c8",
   "metadata": {},
   "source": [
    "### Select a set of `training` images, and build the `training_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `training_image_dataset` variable. First, we load all the images into the `training_image_dictionary::Dict{Int64, Array{Gray{N0f8},3}}`, and then convert these to a vector vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7edf7d-4bf1-468b-9c15-dc847a3c4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 3000;\n",
    "number_digit_array = range(0,length=10,step=1) |> collect;\n",
    "training_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_training_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ 1:number_of_training_examples\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,fileindex] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    training_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c42c51-185f-48c6-8e28-c03f3c6fce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;10;10;10m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;13;13;13m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;9;9;9m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;7;7;7m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;17;17;17m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;5;5;5m██\u001b[38;2;9;9;9m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;17;17;17m██\u001b[38;2;11;11;11m██\u001b[38;2;0;0;0m██\u001b[38;2;5;5;5m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;10;10;10m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;6;6;6m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;11;11;11m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;11;11;11m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;10;10;10m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;14;14;14m██\u001b[38;2;17;17;17m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;16;16;16m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;20;20;20m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;36;36;36m██\u001b[38;2;155;155;155m██\u001b[38;2;218;218;218m██\u001b[38;2;255;255;255m██\u001b[38;2;246;246;246m██\u001b[38;2;255;255;255m██\u001b[38;2;244;244;244m██\u001b[38;2;255;255;255m██\u001b[38;2;218;218;218m██\u001b[38;2;112;112;112m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;16;16;16m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;21;21;21m██\u001b[38;2;149;149;149m██\u001b[38;2;208;208;208m██\u001b[38;2;255;255;255m██\u001b[38;2;234;234;234m██\u001b[38;2;255;255;255m██\u001b[38;2;248;248;248m██\u001b[38;2;255;255;255m██\u001b[38;2;255;255;255m██\u001b[38;2;248;248;248m██\u001b[38;2;255;255;255m██\u001b[38;2;231;231;231m██\u001b[38;2;114;114;114m██\u001b[38;2;11;11;11m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;18;18;18m██\u001b[38;2;139;139;139m██\u001b[38;2;237;237;237m██\u001b[38;2;255;255;255m██\u001b[38;2;238;238;238m██\u001b[38;2;163;163;163m██\u001b[38;2;241;241;241m██\u001b[38;2;238;238;238m██\u001b[38;2;220;220;220m██\u001b[38;2;214;214;214m██\u001b[38;2;182;182;182m██\u001b[38;2;244;244;244m██\u001b[38;2;254;254;254m██\u001b[38;2;175;175;175m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;9;9;9m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;10;10;10m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;75;75;75m██\u001b[38;2;255;255;255m██\u001b[38;2;249;249;249m██\u001b[38;2;186;186;186m██\u001b[38;2;44;44;44m██\u001b[38;2;1;1;1m██\u001b[38;2;49;49;49m██\u001b[38;2;36;36;36m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;120;120;120m██\u001b[38;2;255;255;255m██\u001b[38;2;241;241;241m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;14;14;14m██\u001b[38;2;129;129;129m██\u001b[38;2;250;250;250m██\u001b[38;2;255;255;255m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;10;10;10m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;111;111;111m██\u001b[38;2;255;255;255m██\u001b[38;2;203;203;203m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;149;149;149m██\u001b[38;2;254;254;254m██\u001b[38;2;251;251;251m██\u001b[38;2;0;0;0m██\u001b[38;2;17;17;17m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;17;17;17m██\u001b[38;2;0;0;0m██\u001b[38;2;9;9;9m██\u001b[38;2;5;5;5m██\u001b[38;2;158;158;158m██\u001b[38;2;252;252;252m██\u001b[38;2;190;190;190m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;109;109;109m██\u001b[38;2;255;255;255m██\u001b[38;2;255;255;255m██\u001b[38;2;50;50;50m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;24;24;24m██\u001b[38;2;231;231;231m██\u001b[38;2;248;248;248m██\u001b[38;2;185;185;185m██\u001b[38;2;2;2;2m██\u001b[38;2;1;1;1m██\u001b[38;2;6;6;6m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;46;46;46m██\u001b[38;2;221;221;221m██\u001b[38;2;255;255;255m██\u001b[38;2;254;254;254m██\u001b[38;2;106;106;106m██\u001b[38;2;9;9;9m██\u001b[38;2;3;3;3m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;13;13;13m██\u001b[38;2;80;80;80m██\u001b[38;2;255;255;255m██\u001b[38;2;254;254;254m██\u001b[38;2;129;129;129m██\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;5;5;5m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;9;9;9m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;103;103;103m██\u001b[38;2;242;242;242m██\u001b[38;2;255;255;255m██\u001b[38;2;238;238;238m██\u001b[38;2;219;219;219m██\u001b[38;2;162;162;162m██\u001b[38;2;140;140;140m██\u001b[38;2;64;64;64m██\u001b[38;2;23;23;23m██\u001b[38;2;179;179;179m██\u001b[38;2;253;253;253m██\u001b[38;2;240;240;240m██\u001b[38;2;47;47;47m██\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;108;108;108m██\u001b[38;2;240;240;240m██\u001b[38;2;255;255;255m██\u001b[38;2;255;255;255m██\u001b[38;2;236;236;236m██\u001b[38;2;255;255;255m██\u001b[38;2;253;253;253m██\u001b[38;2;255;255;255m██\u001b[38;2;248;248;248m██\u001b[38;2;249;249;249m██\u001b[38;2;148;148;148m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;6;6;6m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;23;23;23m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;11;11;11m██\u001b[38;2;6;6;6m██\u001b[38;2;141;141;141m██\u001b[38;2;247;247;247m██\u001b[38;2;243;243;243m██\u001b[38;2;255;255;255m██\u001b[38;2;247;247;247m██\u001b[38;2;253;253;253m██\u001b[38;2;246;246;246m██\u001b[38;2;31;31;31m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;5;5;5m██\u001b[38;2;2;2;2m██\u001b[38;2;46;46;46m██\u001b[38;2;239;239;239m██\u001b[38;2;245;245;245m██\u001b[38;2;254;254;254m██\u001b[38;2;218;218;218m██\u001b[38;2;240;240;240m██\u001b[38;2;255;255;255m██\u001b[38;2;202;202;202m██\u001b[38;2;76;76;76m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;4;4;4m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;48;48;48m██\u001b[38;2;255;255;255m██\u001b[38;2;255;255;255m██\u001b[38;2;218;218;218m██\u001b[38;2;147;147;147m██\u001b[38;2;16;16;16m██\u001b[38;2;79;79;79m██\u001b[38;2;249;249;249m██\u001b[38;2;252;252;252m██\u001b[38;2;237;237;237m██\u001b[38;2;71;71;71m██\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;3;3;3m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;18;18;18m██\u001b[38;2;97;97;97m██\u001b[38;2;252;252;252m██\u001b[38;2;249;249;249m██\u001b[38;2;183;183;183m██\u001b[38;2;26;26;26m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;12;12;12m██\u001b[38;2;62;62;62m██\u001b[38;2;210;210;210m██\u001b[38;2;255;255;255m██\u001b[38;2;233;233;233m██\u001b[38;2;19;19;19m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;94;94;94m██\u001b[38;2;255;255;255m██\u001b[38;2;236;236;236m██\u001b[38;2;192;192;192m██\u001b[38;2;30;30;30m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;76;76;76m██\u001b[38;2;255;255;255m██\u001b[38;2;240;240;240m██\u001b[38;2;169;169;169m██\u001b[38;2;0;0;0m██\u001b[38;2;13;13;13m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;1;1;1m██\u001b[38;2;1;1;1m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;5;5;5m██\u001b[38;2;8;8;8m██\u001b[38;2;205;205;205m██\u001b[38;2;255;255;255m██\u001b[38;2;213;213;213m██\u001b[38;2;19;19;19m██\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;14;14;14m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;161;161;161m██\u001b[38;2;252;252;252m██\u001b[38;2;200;200;200m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;2;2;2m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;2;2;2m██\u001b[38;2;8;8;8m██\u001b[38;2;12;12;12m██\u001b[38;2;240;240;240m██\u001b[38;2;255;255;255m██\u001b[38;2;105;105;105m██\u001b[38;2;0;0;0m██\u001b[38;2;12;12;12m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;21;21;21m██\u001b[38;2;179;179;179m██\u001b[38;2;255;255;255m██\u001b[38;2;223;223;223m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;1;1;1m██\u001b[38;2;1;1;1m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;5;5;5m██\u001b[38;2;9;9;9m██\u001b[38;2;200;200;200m██\u001b[38;2;248;248;248m██\u001b[38;2;249;249;249m██\u001b[38;2;162;162;162m██\u001b[38;2;48;48;48m██\u001b[38;2;34;34;34m██\u001b[38;2;0;0;0m██\u001b[38;2;9;9;9m██\u001b[38;2;26;26;26m██\u001b[38;2;106;106;106m██\u001b[38;2;215;215;215m██\u001b[38;2;255;255;255m██\u001b[38;2;244;244;244m██\u001b[38;2;173;173;173m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;2;2;2m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;3;3;3m██\u001b[38;2;20;20;20m██\u001b[38;2;206;206;206m██\u001b[38;2;250;250;250m██\u001b[38;2;242;242;242m██\u001b[38;2;253;253;253m██\u001b[38;2;255;255;255m██\u001b[38;2;193;193;193m██\u001b[38;2;201;201;201m██\u001b[38;2;255;255;255m██\u001b[38;2;240;240;240m██\u001b[38;2;252;252;252m██\u001b[38;2;249;249;249m██\u001b[38;2;215;215;215m██\u001b[38;2;10;10;10m██\u001b[38;2;10;10;10m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;21;21;21m██\u001b[38;2;87;87;87m██\u001b[38;2;181;181;181m██\u001b[38;2;246;246;246m██\u001b[38;2;255;255;255m██\u001b[38;2;255;255;255m██\u001b[38;2;254;254;254m██\u001b[38;2;255;255;255m██\u001b[38;2;243;243;243m██\u001b[38;2;186;186;186m██\u001b[38;2;103;103;103m██\u001b[38;2;33;33;33m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;8;8;8m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;4;4;4m██\u001b[38;2;5;5;5m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;6;6;6m██\u001b[38;2;16;16;16m██\u001b[38;2;1;1;1m██\u001b[38;2;7;7;7m██\u001b[38;2;8;8;8m██\u001b[38;2;2;2;2m██\u001b[38;2;0;0;0m██\u001b[38;2;1;1;1m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m\n",
       "\u001b[0m\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;7;7;7m██\u001b[38;2;12;12;12m██\u001b[38;2;9;9;9m██\u001b[38;2;3;3;3m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;8;8;8m██\u001b[38;2;10;10;10m██\u001b[38;2;7;7;7m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[38;2;0;0;0m██\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_image_dictionary[8][:,:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576da3f-8165-4ee2-90a0-6650704b08cd",
   "metadata": {},
   "source": [
    "Next, we take the images and vectorize them. Each $N\\times{N}$ image array is linearized, i.e., the $N\\times{N}$ array containing the grayscale values at each pixel is converted to an $N^{2}$ vector of values. What image class, i.e., what number it represents, is then converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). The converted data is stored in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable.\n",
    "* __What's the deal with Float32__? Most neural network libraries (or other machine learning calculations) use `Float32` (or lower) to save memory space because of the large number of parameters associated with the network. Additionally, model training is often carried out using specialized hardware [such as Graphical Processing Units (GPUs)](https://www.nvidia.com/en-us/data-center/h100/) which has different memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1554fa2-cfeb-4776-a186-be578f27fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_dataset = let\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array);\n",
    "        X = training_image_dictionary[i];\n",
    "        \n",
    "        for t ∈ 1:number_of_training_examples\n",
    "            D = Array{Float32,1}(undef, 28*28);\n",
    "            linearindex = 1;\n",
    "            for row ∈ 1:28\n",
    "                for col ∈ 1:28\n",
    "                    D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                    linearindex+=1;\n",
    "                end\n",
    "            end\n",
    "    \n",
    "            training_tuple = (D,Y);\n",
    "            push!(training_image_dataset,training_tuple);\n",
    "        end\n",
    "    end\n",
    "    training_image_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acf99db-760b-405c-a494-dac537ddda6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Bool[0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_image_dataset[6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a98b225-cf20-47f7-8775-7eae9d1bf32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element OneHotVector(::UInt32) with eltype Bool:\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " 1\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(5, number_digit_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28bf4b-8671-4975-bf37-95d3b0d8a072",
   "metadata": {},
   "source": [
    "### Select a set of `test` images, and build the `testing_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `testing_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ad3577-cdd3-491b-8b14-636e0c913c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_examples = 500;\n",
    "testing_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_test_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ (number_of_training_examples + 1):(number_of_training_examples+number_of_test_examples)\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,imagecount] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    testing_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1c5708-f557-4625-899e-24bb8605f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = testing_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_test_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        testing_tuple = (D,Y);\n",
    "        push!(testing_image_dataset, testing_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499f7b-5bd3-4a58-939c-633f9831df09",
   "metadata": {},
   "source": [
    "## Task 1: Setup the model structure and training\n",
    "In this task, we'll construct a model and then train the model, i.e., learn the model parameters, using example images encoded in the the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}`. This is an example of _supervised batch learning_, i.e., the model learns its parameters on a fixed set of labeled examples. There is no update rule like an _online_ learning approach; new data cannot be easily included (once we have the parameters, we have them).\n",
    "\n",
    "Let's start by getting the `number_of_input_states::Int64`, this will be the input dimension of the first layer in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f7f536e-0bc5-43c1-8258-7ab7e13ead59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_input_states = length(training_image_dataset[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8b2-fde1-4a36-8a32-1f4d59ba39c7",
   "metadata": {},
   "source": [
    "Then, we build an empty model with default (random) parameter values but a fixed structure, i.e., the number and dimension of the layers, and the activation functions for each layer are specified when we build the model (but we'll update the parameters during training).\n",
    "* We use [the Flux.jl machine learning library](https://github.com/FluxML/Flux.jl) to construct the neural network model. The model will have three layers: the input layer is a `784` $\\times$ `512` layer with [relu activation functions](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), the hidden layer is a `512` $\\times$ `10` layer and the output layer is the [softmax function](https://en.wikipedia.org/wiki/Softmax_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbf74b6b-8aea-494b-b805-159d061acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxNeuralNetworkModel;\n",
    "MyModel() = MyFluxNeuralNetworkModel(\n",
    "    Chain(\n",
    "        Dense(number_of_input_states, 512, relu),  \n",
    "        Dense(512, 10, relu),\n",
    "        NNlib.softmax)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f73bc-28ba-4a9e-af0b-ebc0d42392b0",
   "metadata": {},
   "source": [
    "Next, specify the `loss` function that we will minimize to to esimate the model parameters. In this cross we choose a loss function that is appropriate for a multiclass classification problem, namely a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e432c99f-b8a5-4159-b1f3-5b77667c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a loss function -\n",
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a113-f473-4c02-a409-da18ccb088e7",
   "metadata": {},
   "source": [
    "Then, let's specify which [Gradient descent method]() we will use to search parameter space and estimate the set of parameters that minimizes the `loss` function specified above. \n",
    "* In this case, we'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will eventually get passed to the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "368796dc-7bd9-4c3a-8d88-4092820393f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.01; # learning rate\n",
    "β = 0.90; # momentum parameter\n",
    "opt_state = Flux.setup(Momentum(λ, β), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57be96-4f45-4536-8e86-1d57a2caa24f",
   "metadata": {},
   "source": [
    "We are now ready to train the model. If the `should_we_train` flag is true, then we use the [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy). \n",
    "* Because of the error landscape's non-convex nature, we have to start from many different locations. We do `number_of_epochs` passes through the data, i.e., a forward pass for prediction and a backpropagation step for parameter updates.\n",
    "* Training takes a long time. Thus, for each complete pass through the data, i.e., for each `epoch,` we save a `tmp` file holding the network state... just in case of `BOOOOOOOOM.`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0e9b70-f312-4e3e-8693-c8d3350a4804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 512, relu),              \u001b[90m# 401_920 parameters\u001b[39m\n",
       "  Dense(512 => 10, relu),               \u001b[90m# 5_130 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m407_050 parameters, 1.553 MiB."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "should_we_train = false; # set this flag to {true | false}\n",
    "if (should_we_train == true)\n",
    "    number_of_epochs = 250;\n",
    "    for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(model, training_image_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "    \n",
    "        # output some stuff -\n",
    "        ridx = rand(1:number_of_training_examples);\n",
    "        test_x, test_y = training_image_dataset[ridx][1], training_image_dataset[ridx][2];\n",
    "        l = loss(model(test_x), test_y);\n",
    "        println(\"Training example: $(ridx) has loss = $(l) in epoch $(i)\");\n",
    "    \n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(\"tmp-model-training-checkpoint.jld2\", model_state = Flux.state(model))    \n",
    "    end\n",
    "else\n",
    "    # if we don't train: load up a previous model\n",
    "    model_state = JLD2.load(\"model-state-T3000-P500-E250-N512.jld2\", \"model_state\");\n",
    "    Flux.loadmodel!(model, model_state);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d572-aa6b-4dda-88ea-c52d2a3bc495",
   "metadata": {},
   "source": [
    "## Task 2: How well does the model predict unseen versus observed images?\n",
    "In this task, we'll check the generalization of the network, i.e., how well does it do on data it has not seen. One of the challenges with [Artifical Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) is the lack of generalizability, i.e., they _may not_ perform well on data the model has not seen. Let's explore this question:\n",
    "* To begin with, let's compute the fraction of the `training data` that is not correctly classified. This will help us understand how many of the `N` training samples we get correct and how many we get wrong. We expect to be _mostly correct_ on this data.\n",
    "* Next, we'll do the same thing but with the `test data,` i.e., data the model has never seen. We expect the correct prediction fraction to be less than the equivalent training value on the `test data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98398ccb-1ec6-40c7-9586-f3ad475011f1",
   "metadata": {},
   "source": [
    "### Compute the correct prediction fraction for the `training` and `test` datasets\n",
    "Process each image in the `testing_image_dataset` dataset. Pass the pixel data from the image into the `model` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels. If they argree, we update the `S` variable (a running count of the number of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c67a99f9-5efe-48de-a8f8-d9962dff24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 89.92999999999999%\n"
     ]
    }
   ],
   "source": [
    "S_training = 0;\n",
    "for i ∈ eachindex(training_image_dataset)\n",
    "    \n",
    "    x = training_image_dataset[i][1];\n",
    "    y = training_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,number_digit_array)\n",
    "    y == ŷ ? S_training +=1 : nothing\n",
    "end\n",
    "correct_prediction_training = (S_training/length(training_image_dataset))*100;\n",
    "println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a021a683-8441-469d-8981-0c48800cf92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction on the test data: 89.64%\n"
     ]
    }
   ],
   "source": [
    "S_testing = 0;\n",
    "for i ∈ eachindex(testing_image_dataset)\n",
    "    \n",
    "    x = testing_image_dataset[i][1];\n",
    "    y = testing_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    y == ŷ ? S_testing+=1 : nothing\n",
    "end\n",
    "correct_prediction_test = (S_testing/length(testing_image_dataset))*100;\n",
    "println(\"Correct prediction on the test data: $(correct_prediction_test)%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535531c-5361-4a4a-8f52-d8170be52f5b",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Let's do a few manual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "789967b5-7126-40a3-a57c-649c1cdff027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Matrix{Float32}:\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       " 1.0  1.0\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       " 0.0  0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    N = length(testing_image_dataset);\n",
    "    i = rand(1:N)\n",
    "    x = testing_image_dataset[i][1];\n",
    "    y = testing_image_dataset[i][2];\n",
    "    ŷ = model(x);\n",
    "    [y ŷ]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6734a29-49fa-4f33-884b-ff0f47ef078d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
