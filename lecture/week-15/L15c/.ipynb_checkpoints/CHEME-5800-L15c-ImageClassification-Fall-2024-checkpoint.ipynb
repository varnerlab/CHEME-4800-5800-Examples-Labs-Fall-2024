{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301524c-c75f-4769-9421-b31c4fc6f0e0",
   "metadata": {},
   "source": [
    "# Example: Develop a Multiclass Artificial Neural Network Image Classifier\n",
    "This example will familiarize students with constructing, training, and testing a simple [Feed-Forward Artificial Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network) that will classify images of handwritten numbers between `0,...,9` taken from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). Each digit between `0` and `9` has approximately 5000 example images, each of which is a `28`$\\times$`28` grayscale image; thus, each image has `784` pixels.  \n",
    "\n",
    "* In this example, we'll use the [Flux.jl package](https://github.com/FluxML/Flux.jl) to build, train, and test our image classifier. However, there are two excellent libraries for ANNs in Python (sort of), namely the [PyTorch library](https://pytorch.org/) from the [AI group at META](https://ai.meta.com/meta-ai/) and the [TensorFlow library](https://www.tensorflow.org/) developed by [Google](https://research.google/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0043b-6b9a-4851-b1ff-ce4fef45e091",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This example requires several external libraries and a function to compute the outer product. Let's download and install these packages and call our `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc952ea-d4e4-449b-aec7-ddc44d741972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-15/L15c/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-15/L15c/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f403b43-7d09-4058-a96c-aae3bbfca8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension(file::String) = file[findlast(==('.'), file)+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc914a79-bf7f-4889-b1d1-54003da268d8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before training and testing the `ANN,` we need to construct two datasets. First, we need to build a `training dataset` of images that we will use to estimate the model parameters. We'll save training data in the `training_image_dataset` variable. Next, we'll construct a `test dataset,` which we'll use to see how well our `ANN` predicts data it has never seen. We'll save this data in the `testing_image_dataset` variable.\n",
    "* Both the `training_image_dataset` and `testing_image_dataset` will be of type `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` where the first element is the input data `x.` The second element is the `label,` i.e., whether the image corresponds to `0,....,9`.\n",
    "* The `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` type has a couple of weird features. First, notice that the floating point is `Float32`, not the default `Float64`. Next, the labels are [One Hot ecoded](https://en.wikipedia.org/wiki/One-hot). Finally, the input data `x` is a Vector, not a Matrix (even though the original image is a matrix of `Gray` values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583d83-6353-450f-9a6f-2ec5b44185c8",
   "metadata": {},
   "source": [
    "### Select a set of `training` images, and build the `training_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `training_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7edf7d-4bf1-468b-9c15-dc847a3c4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 3000;\n",
    "number_digit_array = range(0,length=10,step=1) |> collect;\n",
    "training_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_training_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ 1:number_of_training_examples\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,fileindex] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    training_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1554fa2-cfeb-4776-a186-be578f27fffd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = training_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_training_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        training_tuple = (D,Y);\n",
    "        push!(training_image_dataset,training_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28bf4b-8671-4975-bf37-95d3b0d8a072",
   "metadata": {},
   "source": [
    "### Select a set of `test` images, and build the `testing_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `testing_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ad3577-cdd3-491b-8b14-636e0c913c8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_test_examples = 500;\n",
    "testing_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_test_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ (number_of_training_examples + 1):(number_of_training_examples+number_of_test_examples)\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,imagecount] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    testing_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c5708-f557-4625-899e-24bb8605f641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "testing_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = testing_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_test_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        testing_tuple = (D,Y);\n",
    "        push!(testing_image_dataset, testing_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fb784-2674-4675-90a8-f9fda79525ed",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6bae7b-1671-4bfa-8f84-89ca84152187",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_we_train = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499f7b-5bd3-4a58-939c-633f9831df09",
   "metadata": {},
   "source": [
    "## Setup the model structure and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7f536e-0bc5-43c1-8258-7ab7e13ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_input_states = length(training_image_dataset[1][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8b2-fde1-4a36-8a32-1f4d59ba39c7",
   "metadata": {},
   "source": [
    "Let's build an empty model with default parameter values but a fixed structure, i.e., the number and dimension of the layers, and the activation functions for each layer are specified when we build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf74b6b-8aea-494b-b805-159d061acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxNeuralNetworkModel;\n",
    "MyModel() = MyFluxNeuralNetworkModel(\n",
    "    Chain(\n",
    "        Dense(number_of_input_states, 512, relu),  \n",
    "        Dense(512, 10, relu),\n",
    "        NNlib.softmax)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0a8ea72-bea5-404a-ac61-12b4893a3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model_state = JLD2.load(\"tmp-model-training-checkpoint.jld2\", \"model_state\");\n",
    "# Flux.loadmodel!(model, model_state);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f73bc-28ba-4a9e-af0b-ebc0d42392b0",
   "metadata": {},
   "source": [
    "Next, specify the `loss` function that we will minimize to to esimate the model parameters. In this cross we choose a loss function that is appropriate for a multiclass classification problem, namely a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e432c99f-b8a5-4159-b1f3-5b77667c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a loss function -\n",
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a113-f473-4c02-a409-da18ccb088e7",
   "metadata": {},
   "source": [
    "Then, let's specify which [Gradient descent method]() we will use to search parameter space and estimate the set of parameters that minimizes the `loss` function specified above. \n",
    "* In this case, we'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will eventually get passed to the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "368796dc-7bd9-4c3a-8d88-4092820393f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.01; # learning rate\n",
    "β = 0.90; # momentum parameter\n",
    "opt_state = Flux.setup(Momentum(λ, β), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57be96-4f45-4536-8e86-1d57a2caa24f",
   "metadata": {},
   "source": [
    "We are now ready to train the model. If the `should_we_train` flag is true, then we use the [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy). \n",
    "* Because of the error landscape's non-convex nature, we have to start from many different locations. We do `number_of_epochs` passes through the data, i.e., a forward pass for prediction and a backpropagation step for parameter updates.\n",
    "* Training takes a long time. Thus, for each complete pass through the data, i.e., for each `epoch,` we save a `tmp` file holding the network state... just in case of `BOOOOOOOOM.`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0e9b70-f312-4e3e-8693-c8d3350a4804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 512, relu),              \u001b[90m# 401_920 parameters\u001b[39m\n",
       "  Dense(512 => 10, relu),               \u001b[90m# 5_130 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m407_050 parameters, 1.553 MiB."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (should_we_train == true)\n",
    "    number_of_epochs = 250;\n",
    "    for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(model, training_image_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "    \n",
    "        # output some stuff -\n",
    "        ridx = rand(1:number_of_training_examples);\n",
    "        test_x, test_y = training_image_dataset[ridx][1], training_image_dataset[ridx][2];\n",
    "        l = loss(model(test_x), test_y);\n",
    "        println(\"Training example: $(ridx) has loss = $(l) in epoch $(i)\");\n",
    "    \n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(\"tmp-model-training-checkpoint.jld2\", model_state = Flux.state(model))    \n",
    "    end\n",
    "else\n",
    "    model_state = JLD2.load(\"model-state-T3000-P500-E250-N512.jld2\", \"model_state\");\n",
    "    Flux.loadmodel!(model, model_state);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d572-aa6b-4dda-88ea-c52d2a3bc495",
   "metadata": {},
   "source": [
    "## How well does the model predict unseen versus observed images?\n",
    "One of the challenges with [Artifical Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) is the lack of generalizability, i.e., they _may not_ perform well on data the model has not seen. Let's explore this question:\n",
    "* To begin with, let's compute the fraction of the `training data` that is not correctly classified. This will help us understand how many of the `N` training samples we get correct and how many we get wrong. We expect to be _mostly correct_ on this data.\n",
    "* Next, we'll do the same thing but with the `test data,` i.e., data the model has never seen. We expect the correct prediction fraction to be less than the equivalent training value on the `test data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98398ccb-1ec6-40c7-9586-f3ad475011f1",
   "metadata": {},
   "source": [
    "### Compute the correct prediction fraction for the `training` and `test` datasets\n",
    "Process each image in the `testing_image_dataset` dataset. Pass the pixel data from the image into the `model` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels. If they argree, we update the `S` variable (a running count of the number of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c67a99f9-5efe-48de-a8f8-d9962dff24a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 89.92999999999999%\n"
     ]
    }
   ],
   "source": [
    "S_training = 0;\n",
    "for i ∈ eachindex(training_image_dataset)\n",
    "    \n",
    "    x = training_image_dataset[i][1];\n",
    "    y = training_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,number_digit_array)\n",
    "    y == ŷ ? S_training +=1 : nothing\n",
    "end\n",
    "correct_prediction_training = (S_training/length(training_image_dataset))*100;\n",
    "println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a021a683-8441-469d-8981-0c48800cf92b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction on the test data: 89.64%\n"
     ]
    }
   ],
   "source": [
    "S_testing = 0;\n",
    "for i ∈ eachindex(testing_image_dataset)\n",
    "    \n",
    "    x = testing_image_dataset[i][1];\n",
    "    y = testing_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    y == ŷ ? S_testing+=1 : nothing\n",
    "end\n",
    "correct_prediction_test = (S_testing/length(testing_image_dataset))*100;\n",
    "println(\"Correct prediction on the test data: $(correct_prediction_test)%\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
