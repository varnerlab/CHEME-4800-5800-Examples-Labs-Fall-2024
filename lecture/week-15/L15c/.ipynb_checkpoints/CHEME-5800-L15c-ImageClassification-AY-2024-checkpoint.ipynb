{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301524c-c75f-4769-9421-b31c4fc6f0e0",
   "metadata": {},
   "source": [
    "# Example: Develop a Multiclass Artificial Neural Network Image Classifier\n",
    "This example will familiarize students with constructing, training, and testing a simple [Feed-Forward Artificial Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network) that will classify images of handwritten numbers between `0,...,9` taken from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). Each digit between `0` and `9` has approximately 5000 example images, each of which is a `28`$\\times$`28` grayscale image; thus, each image has `784` pixels.  \n",
    "\n",
    "* In this example, we'll use the [Flux.jl package](https://github.com/FluxML/Flux.jl) to build, train, and test our image classifier. However, there are two excellent libraries for ANNs in Python (sort of), namely the [PyTorch library](https://pytorch.org/) from the [AI group at META](https://ai.meta.com/meta-ai/) and the [TensorFlow library](https://www.tensorflow.org/) developed by [Google](https://research.google/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0043b-6b9a-4851-b1ff-ce4fef45e091",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This example requires several external libraries and a function to compute the outer product. Let's download and install these packages and call our `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc952ea-d4e4-449b-aec7-ddc44d741972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-15/L15c/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-15/L15c/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f403b43-7d09-4058-a96c-aae3bbfca8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension(file::String) = file[findlast(==('.'), file)+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc914a79-bf7f-4889-b1d1-54003da268d8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before training and testing the `ANN,` we need to construct two datasets. First, we need to build a `training dataset` of images that we will use to estimate the model parameters. We'll save training data in the `training_image_dataset` variable. Next, we'll construct a `test dataset,` which we'll use to see how well our `ANN` predicts data it has never seen. We'll save this data in the `testing_image_dataset` variable.\n",
    "* Both the `training_image_dataset` and `testing_image_dataset` will be of type `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` where the first element is the input data `x.` The second element is the `label,` i.e., whether the image corresponds to `0,....,9`.\n",
    "* The `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` type has a couple of weird features. First, notice that the floating point is `Float32`, not the default `Float64`. Next, the labels are [One Hot ecoded](https://en.wikipedia.org/wiki/One-hot). Finally, the input data `x` is a Vector, not a Matrix (even though the original image is a matrix of `Gray` values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583d83-6353-450f-9a6f-2ec5b44185c8",
   "metadata": {},
   "source": [
    "### Select a set of `training` images, and build the `training_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `training_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7edf7d-4bf1-468b-9c15-dc847a3c4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 3000;\n",
    "number_digit_array = range(0,length=10,step=1) |> collect;\n",
    "training_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_training_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ 1:number_of_training_examples\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,fileindex] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    training_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1554fa2-cfeb-4776-a186-be578f27fffd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = training_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_training_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        training_tuple = (D,Y);\n",
    "        push!(training_image_dataset,training_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28bf4b-8671-4975-bf37-95d3b0d8a072",
   "metadata": {},
   "source": [
    "### Select a set of `test` images, and build the `testing_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `testing_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ad3577-cdd3-491b-8b14-636e0c913c8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_test_examples = 500;\n",
    "testing_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_test_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ (number_of_training_examples + 1):(number_of_training_examples+number_of_test_examples)\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,imagecount] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    testing_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c5708-f557-4625-899e-24bb8605f641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "testing_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = testing_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_test_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        testing_tuple = (D,Y);\n",
    "        push!(testing_image_dataset, testing_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fb784-2674-4675-90a8-f9fda79525ed",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6bae7b-1671-4bfa-8f84-89ca84152187",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_we_train = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499f7b-5bd3-4a58-939c-633f9831df09",
   "metadata": {},
   "source": [
    "## Setup the model structure and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7f536e-0bc5-43c1-8258-7ab7e13ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_input_states = length(training_image_dataset[1][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8b2-fde1-4a36-8a32-1f4d59ba39c7",
   "metadata": {},
   "source": [
    "Let's build an empty model with default parameter values but a fixed structure, i.e., the number and dimension of the layers, and the activation functions for each layer are specified when we build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf74b6b-8aea-494b-b805-159d061acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxNeuralNetworkModel;\n",
    "MyModel() = MyFluxNeuralNetworkModel(\n",
    "    Chain(\n",
    "        Dense(number_of_input_states, 512, relu),  \n",
    "        Dense(512, 10, relu),\n",
    "        NNlib.softmax)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0a8ea72-bea5-404a-ac61-12b4893a3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model_state = JLD2.load(\"tmp-model-training-checkpoint.jld2\", \"model_state\");\n",
    "# Flux.loadmodel!(model, model_state);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f73bc-28ba-4a9e-af0b-ebc0d42392b0",
   "metadata": {},
   "source": [
    "Next, specify the `loss` function that we will minimize to to esimate the model parameters. In this cross we choose a loss function that is appropriate for a multiclass classification problem, namely a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e432c99f-b8a5-4159-b1f3-5b77667c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a loss function -\n",
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a113-f473-4c02-a409-da18ccb088e7",
   "metadata": {},
   "source": [
    "Then, let's specify which [Gradient descent method]() we will use to search parameter space and estimate the set of parameters that minimizes the `loss` function specified above. \n",
    "* In this case, we'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will eventually get passed to the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "368796dc-7bd9-4c3a-8d88-4092820393f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.01; # learning rate\n",
    "β = 0.90; # momentum parameter\n",
    "opt_state = Flux.setup(Momentum(λ, β), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57be96-4f45-4536-8e86-1d57a2caa24f",
   "metadata": {},
   "source": [
    "We are now ready to train the model. If the `should_we_train` flag is true, then we use the [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy). \n",
    "* Because of the error landscape's non-convex nature, we have to start from many different locations. We do `number_of_epochs` passes through the data, i.e., a forward pass for prediction and a backpropagation step for parameter updates.\n",
    "* Training takes a long time. Thus, for each complete pass through the data, i.e., for each `epoch,` we save a `tmp` file holding the network state... just in case of `BOOOOOOOOM.`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0e9b70-f312-4e3e-8693-c8d3350a4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example: 211 has loss = 2.4611502 in epoch 1\n",
      "Training example: 1954 has loss = 1.46115 in epoch 2\n",
      "Training example: 1895 has loss = 1.46115 in epoch 3\n",
      "Training example: 2406 has loss = 1.46115 in epoch 4\n",
      "Training example: 354 has loss = 1.46115 in epoch 5\n",
      "Training example: 774 has loss = 1.46115 in epoch 6\n",
      "Training example: 479 has loss = 1.46115 in epoch 7\n",
      "Training example: 327 has loss = 1.46115 in epoch 8\n",
      "Training example: 2906 has loss = 1.46115 in epoch 9\n",
      "Training example: 805 has loss = 1.46115 in epoch 10\n",
      "Training example: 2639 has loss = 1.46115 in epoch 11\n",
      "Training example: 564 has loss = 1.46115 in epoch 12\n",
      "Training example: 1825 has loss = 1.46115 in epoch 13\n",
      "Training example: 2990 has loss = 1.46115 in epoch 14\n",
      "Training example: 995 has loss = 1.46115 in epoch 15\n",
      "Training example: 2395 has loss = 1.46115 in epoch 16\n",
      "Training example: 1811 has loss = 1.46115 in epoch 17\n",
      "Training example: 2143 has loss = 1.46115 in epoch 18\n",
      "Training example: 532 has loss = 1.46115 in epoch 19\n",
      "Training example: 2741 has loss = 1.46115 in epoch 20\n",
      "Training example: 2559 has loss = 1.46115 in epoch 21\n",
      "Training example: 866 has loss = 1.46115 in epoch 22\n",
      "Training example: 764 has loss = 1.46115 in epoch 23\n",
      "Training example: 29 has loss = 1.46115 in epoch 24\n",
      "Training example: 2576 has loss = 2.4611502 in epoch 25\n",
      "Training example: 2425 has loss = 1.46115 in epoch 26\n",
      "Training example: 1574 has loss = 1.46115 in epoch 27\n",
      "Training example: 502 has loss = 1.46115 in epoch 28\n",
      "Training example: 1169 has loss = 1.46115 in epoch 29\n",
      "Training example: 393 has loss = 1.46115 in epoch 30\n",
      "Training example: 2628 has loss = 1.46115 in epoch 31\n",
      "Training example: 960 has loss = 1.46115 in epoch 32\n",
      "Training example: 2 has loss = 2.4611502 in epoch 33\n",
      "Training example: 2708 has loss = 1.46115 in epoch 34\n",
      "Training example: 5 has loss = 1.46115 in epoch 35\n",
      "Training example: 2173 has loss = 1.46115 in epoch 36\n",
      "Training example: 497 has loss = 1.46115 in epoch 37\n",
      "Training example: 1494 has loss = 1.46115 in epoch 38\n",
      "Training example: 1110 has loss = 1.46115 in epoch 39\n",
      "Training example: 2521 has loss = 1.46115 in epoch 40\n",
      "Training example: 2201 has loss = 1.46115 in epoch 41\n",
      "Training example: 2664 has loss = 1.46115 in epoch 42\n",
      "Training example: 2054 has loss = 1.46115 in epoch 43\n",
      "Training example: 1660 has loss = 1.46115 in epoch 44\n",
      "Training example: 2423 has loss = 1.46115 in epoch 45\n",
      "Training example: 160 has loss = 1.46115 in epoch 46\n",
      "Training example: 2199 has loss = 1.46115 in epoch 47\n",
      "Training example: 2672 has loss = 1.46115 in epoch 48\n",
      "Training example: 1682 has loss = 1.46115 in epoch 49\n",
      "Training example: 2087 has loss = 1.46115 in epoch 50\n",
      "Training example: 1379 has loss = 1.46115 in epoch 51\n",
      "Training example: 2431 has loss = 1.46115 in epoch 52\n",
      "Training example: 446 has loss = 1.46115 in epoch 53\n",
      "Training example: 2582 has loss = 1.46115 in epoch 54\n",
      "Training example: 1553 has loss = 1.46115 in epoch 55\n",
      "Training example: 2046 has loss = 1.46115 in epoch 56\n",
      "Training example: 2967 has loss = 1.46115 in epoch 57\n",
      "Training example: 2698 has loss = 1.46115 in epoch 58\n",
      "Training example: 596 has loss = 1.46115 in epoch 59\n",
      "Training example: 108 has loss = 2.4611502 in epoch 60\n",
      "Training example: 851 has loss = 1.46115 in epoch 61\n",
      "Training example: 2756 has loss = 1.46115 in epoch 62\n",
      "Training example: 1963 has loss = 1.46115 in epoch 63\n",
      "Training example: 2945 has loss = 1.46115 in epoch 64\n",
      "Training example: 2716 has loss = 1.46115 in epoch 65\n",
      "Training example: 2261 has loss = 1.46115 in epoch 66\n",
      "Training example: 1721 has loss = 1.46115 in epoch 67\n",
      "Training example: 2398 has loss = 1.46115 in epoch 68\n",
      "Training example: 796 has loss = 1.46115 in epoch 69\n",
      "Training example: 1163 has loss = 1.46115 in epoch 70\n",
      "Training example: 346 has loss = 1.46115 in epoch 71\n",
      "Training example: 306 has loss = 1.46115 in epoch 72\n",
      "Training example: 2925 has loss = 1.46115 in epoch 73\n",
      "Training example: 1134 has loss = 1.46115 in epoch 74\n",
      "Training example: 835 has loss = 1.46115 in epoch 75\n",
      "Training example: 912 has loss = 1.46115 in epoch 76\n",
      "Training example: 2074 has loss = 1.46115 in epoch 77\n",
      "Training example: 430 has loss = 1.46115 in epoch 78\n",
      "Training example: 2997 has loss = 1.46115 in epoch 79\n",
      "Training example: 2012 has loss = 1.46115 in epoch 80\n",
      "Training example: 640 has loss = 1.46115 in epoch 81\n",
      "Training example: 2755 has loss = 1.46115 in epoch 82\n",
      "Training example: 2578 has loss = 1.46115 in epoch 83\n",
      "Training example: 2952 has loss = 1.46115 in epoch 84\n",
      "Training example: 2048 has loss = 1.46115 in epoch 85\n",
      "Training example: 1511 has loss = 1.46115 in epoch 86\n",
      "Training example: 2222 has loss = 1.46115 in epoch 87\n",
      "Training example: 2384 has loss = 1.46115 in epoch 88\n",
      "Training example: 2401 has loss = 1.46115 in epoch 89\n",
      "Training example: 2050 has loss = 1.46115 in epoch 90\n",
      "Training example: 2823 has loss = 1.46115 in epoch 91\n",
      "Training example: 1847 has loss = 1.46115 in epoch 92\n",
      "Training example: 1476 has loss = 1.46115 in epoch 93\n",
      "Training example: 17 has loss = 1.46115 in epoch 94\n",
      "Training example: 424 has loss = 1.46115 in epoch 95\n",
      "Training example: 1132 has loss = 1.46115 in epoch 96\n",
      "Training example: 2330 has loss = 1.46115 in epoch 97\n",
      "Training example: 583 has loss = 1.46115 in epoch 98\n",
      "Training example: 2417 has loss = 1.46115 in epoch 99\n",
      "Training example: 1511 has loss = 1.46115 in epoch 100\n",
      "Training example: 1037 has loss = 1.46115 in epoch 101\n",
      "Training example: 2214 has loss = 1.46115 in epoch 102\n",
      "Training example: 111 has loss = 1.46115 in epoch 103\n",
      "Training example: 2340 has loss = 1.46115 in epoch 104\n",
      "Training example: 460 has loss = 1.46115 in epoch 105\n",
      "Training example: 130 has loss = 2.4611502 in epoch 106\n",
      "Training example: 2249 has loss = 1.46115 in epoch 107\n",
      "Training example: 1348 has loss = 1.46115 in epoch 108\n",
      "Training example: 1743 has loss = 1.46115 in epoch 109\n",
      "Training example: 1543 has loss = 1.46115 in epoch 110\n",
      "Training example: 405 has loss = 1.46115 in epoch 111\n",
      "Training example: 1279 has loss = 1.46115 in epoch 112\n",
      "Training example: 681 has loss = 1.46115 in epoch 113\n",
      "Training example: 1719 has loss = 1.46115 in epoch 114\n",
      "Training example: 483 has loss = 1.46115 in epoch 115\n",
      "Training example: 175 has loss = 1.46115 in epoch 116\n",
      "Training example: 1419 has loss = 1.46115 in epoch 117\n",
      "Training example: 730 has loss = 1.46115 in epoch 118\n",
      "Training example: 939 has loss = 1.46115 in epoch 119\n",
      "Training example: 56 has loss = 1.46115 in epoch 120\n",
      "Training example: 734 has loss = 1.46115 in epoch 121\n",
      "Training example: 1658 has loss = 1.46115 in epoch 122\n",
      "Training example: 111 has loss = 1.46115 in epoch 123\n",
      "Training example: 250 has loss = 1.46115 in epoch 124\n",
      "Training example: 2014 has loss = 1.46115 in epoch 125\n",
      "Training example: 2712 has loss = 1.46115 in epoch 126\n",
      "Training example: 2420 has loss = 1.46115 in epoch 127\n",
      "Training example: 2158 has loss = 1.46115 in epoch 128\n",
      "Training example: 889 has loss = 2.4611502 in epoch 129\n",
      "Training example: 663 has loss = 1.46115 in epoch 130\n",
      "Training example: 1361 has loss = 1.46115 in epoch 131\n",
      "Training example: 1015 has loss = 1.46115 in epoch 132\n",
      "Training example: 2790 has loss = 1.46115 in epoch 133\n",
      "Training example: 82 has loss = 1.46115 in epoch 134\n",
      "Training example: 1299 has loss = 1.46115 in epoch 135\n",
      "Training example: 2914 has loss = 1.46115 in epoch 136\n",
      "Training example: 431 has loss = 1.46115 in epoch 137\n",
      "Training example: 806 has loss = 1.46115 in epoch 138\n",
      "Training example: 984 has loss = 1.46115 in epoch 139\n",
      "Training example: 2948 has loss = 1.46115 in epoch 140\n",
      "Training example: 939 has loss = 1.46115 in epoch 141\n",
      "Training example: 701 has loss = 1.46115 in epoch 142\n",
      "Training example: 1878 has loss = 1.46115 in epoch 143\n",
      "Training example: 1342 has loss = 1.46115 in epoch 144\n",
      "Training example: 2208 has loss = 1.46115 in epoch 145\n",
      "Training example: 631 has loss = 1.46115 in epoch 146\n",
      "Training example: 436 has loss = 1.46115 in epoch 147\n",
      "Training example: 1004 has loss = 1.46115 in epoch 148\n",
      "Training example: 1112 has loss = 1.46115 in epoch 149\n",
      "Training example: 2829 has loss = 1.46115 in epoch 150\n",
      "Training example: 805 has loss = 1.46115 in epoch 151\n",
      "Training example: 172 has loss = 1.46115 in epoch 152\n",
      "Training example: 1490 has loss = 1.46115 in epoch 153\n",
      "Training example: 1308 has loss = 1.46115 in epoch 154\n",
      "Training example: 1015 has loss = 1.46115 in epoch 155\n",
      "Training example: 2015 has loss = 1.46115 in epoch 156\n",
      "Training example: 1608 has loss = 2.4611502 in epoch 157\n",
      "Training example: 1784 has loss = 1.46115 in epoch 158\n",
      "Training example: 2647 has loss = 1.46115 in epoch 159\n",
      "Training example: 2204 has loss = 1.46115 in epoch 160\n",
      "Training example: 1820 has loss = 1.46115 in epoch 161\n",
      "Training example: 2049 has loss = 1.46115 in epoch 162\n",
      "Training example: 758 has loss = 1.46115 in epoch 163\n",
      "Training example: 1295 has loss = 1.46115 in epoch 164\n",
      "Training example: 1446 has loss = 1.46115 in epoch 165\n",
      "Training example: 1367 has loss = 1.46115 in epoch 166\n",
      "Training example: 858 has loss = 1.46115 in epoch 167\n",
      "Training example: 1941 has loss = 1.46115 in epoch 168\n",
      "Training example: 2042 has loss = 1.46115 in epoch 169\n",
      "Training example: 1799 has loss = 1.46115 in epoch 170\n",
      "Training example: 2231 has loss = 1.46115 in epoch 171\n",
      "Training example: 1278 has loss = 1.46115 in epoch 172\n",
      "Training example: 1338 has loss = 1.46115 in epoch 173\n",
      "Training example: 2288 has loss = 1.46115 in epoch 174\n",
      "Training example: 2287 has loss = 1.46115 in epoch 175\n",
      "Training example: 2938 has loss = 1.46115 in epoch 176\n",
      "Training example: 330 has loss = 1.46115 in epoch 177\n",
      "Training example: 1 has loss = 2.4611502 in epoch 178\n",
      "Training example: 2660 has loss = 1.46115 in epoch 179\n",
      "Training example: 1317 has loss = 1.46115 in epoch 180\n",
      "Training example: 2902 has loss = 1.46115 in epoch 181\n",
      "Training example: 525 has loss = 1.46115 in epoch 182\n",
      "Training example: 344 has loss = 1.46115 in epoch 183\n",
      "Training example: 52 has loss = 1.46115 in epoch 184\n",
      "Training example: 1464 has loss = 1.46115 in epoch 185\n",
      "Training example: 486 has loss = 1.46115 in epoch 186\n",
      "Training example: 526 has loss = 1.46115 in epoch 187\n",
      "Training example: 1801 has loss = 1.46115 in epoch 188\n",
      "Training example: 2173 has loss = 1.46115 in epoch 189\n",
      "Training example: 178 has loss = 1.46115 in epoch 190\n",
      "Training example: 863 has loss = 1.46115 in epoch 191\n",
      "Training example: 1615 has loss = 1.46115 in epoch 192\n",
      "Training example: 789 has loss = 1.46115 in epoch 193\n",
      "Training example: 1422 has loss = 1.46115 in epoch 194\n",
      "Training example: 151 has loss = 1.46115 in epoch 195\n",
      "Training example: 2530 has loss = 1.46115 in epoch 196\n",
      "Training example: 872 has loss = 1.46115 in epoch 197\n",
      "Training example: 1273 has loss = 1.46115 in epoch 198\n",
      "Training example: 1730 has loss = 1.46115 in epoch 199\n",
      "Training example: 175 has loss = 1.46115 in epoch 200\n",
      "Training example: 1296 has loss = 1.46115 in epoch 201\n",
      "Training example: 619 has loss = 1.46115 in epoch 202\n",
      "Training example: 2116 has loss = 1.46115 in epoch 203\n",
      "Training example: 1286 has loss = 1.46115 in epoch 204\n",
      "Training example: 1837 has loss = 1.46115 in epoch 205\n",
      "Training example: 2537 has loss = 1.46115 in epoch 206\n",
      "Training example: 2114 has loss = 1.46115 in epoch 207\n",
      "Training example: 1031 has loss = 1.46115 in epoch 208\n",
      "Training example: 531 has loss = 1.46115 in epoch 209\n",
      "Training example: 1547 has loss = 1.46115 in epoch 210\n",
      "Training example: 96 has loss = 1.46115 in epoch 211\n",
      "Training example: 2486 has loss = 1.46115 in epoch 212\n",
      "Training example: 1829 has loss = 1.46115 in epoch 213\n",
      "Training example: 1897 has loss = 1.46115 in epoch 214\n",
      "Training example: 1946 has loss = 1.46115 in epoch 215\n",
      "Training example: 2987 has loss = 1.46115 in epoch 216\n",
      "Training example: 1226 has loss = 1.46115 in epoch 217\n",
      "Training example: 1159 has loss = 1.46115 in epoch 218\n",
      "Training example: 2444 has loss = 2.4611502 in epoch 219\n",
      "Training example: 198 has loss = 1.46115 in epoch 220\n",
      "Training example: 1530 has loss = 1.46115 in epoch 221\n",
      "Training example: 256 has loss = 1.46115 in epoch 222\n",
      "Training example: 949 has loss = 1.46115 in epoch 223\n",
      "Training example: 748 has loss = 1.46115 in epoch 224\n",
      "Training example: 2679 has loss = 1.46115 in epoch 225\n",
      "Training example: 2966 has loss = 1.46115 in epoch 226\n",
      "Training example: 45 has loss = 1.46115 in epoch 227\n",
      "Training example: 2149 has loss = 1.46115 in epoch 228\n",
      "Training example: 464 has loss = 1.46115 in epoch 229\n",
      "Training example: 2780 has loss = 1.46115 in epoch 230\n",
      "Training example: 216 has loss = 1.46115 in epoch 231\n",
      "Training example: 2661 has loss = 1.46115 in epoch 232\n",
      "Training example: 1769 has loss = 1.46115 in epoch 233\n",
      "Training example: 1824 has loss = 1.46115 in epoch 234\n",
      "Training example: 692 has loss = 1.46115 in epoch 235\n",
      "Training example: 506 has loss = 1.46115 in epoch 236\n",
      "Training example: 2497 has loss = 1.46115 in epoch 237\n",
      "Training example: 2621 has loss = 1.46115 in epoch 238\n",
      "Training example: 202 has loss = 1.46115 in epoch 239\n",
      "Training example: 422 has loss = 1.46115 in epoch 240\n",
      "Training example: 259 has loss = 1.46115 in epoch 241\n",
      "Training example: 20 has loss = 1.46115 in epoch 242\n",
      "Training example: 697 has loss = 1.46115 in epoch 243\n",
      "Training example: 2193 has loss = 1.46115 in epoch 244\n",
      "Training example: 1985 has loss = 1.46115 in epoch 245\n",
      "Training example: 2644 has loss = 1.46115 in epoch 246\n",
      "Training example: 1126 has loss = 1.46115 in epoch 247\n",
      "Training example: 444 has loss = 1.46115 in epoch 248\n",
      "Training example: 2431 has loss = 1.46115 in epoch 249\n",
      "Training example: 2793 has loss = 1.46115 in epoch 250\n"
     ]
    }
   ],
   "source": [
    "if (should_we_train == true)\n",
    "    number_of_epochs = 250;\n",
    "    for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(model, training_image_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "    \n",
    "        # output some stuff -\n",
    "        ridx = rand(1:number_of_training_examples);\n",
    "        test_x, test_y = training_image_dataset[ridx][1], training_image_dataset[ridx][2];\n",
    "        l = loss(model(test_x), test_y);\n",
    "        println(\"Training example: $(ridx) has loss = $(l) in epoch $(i)\");\n",
    "    \n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(\"tmp-model-training-checkpoint.jld2\", model_state = Flux.state(model))    \n",
    "    end\n",
    "else\n",
    "    model_state = JLD2.load(\"model-state-T3000-P500-E250-N512.jld2\", \"model_state\");\n",
    "    Flux.loadmodel!(model, model_state);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d572-aa6b-4dda-88ea-c52d2a3bc495",
   "metadata": {},
   "source": [
    "## How well does the model predict unseen versus observed images?\n",
    "One of the challenges with [Artifical Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) is the lack of generalizability, i.e., they _may not_ perform well on data the model has not seen. Let's explore this question:\n",
    "* To begin with, let's compute the fraction of the `training data` that is not correctly classified. This will help us understand how many of the `N` training samples we get correct and how many we get wrong. We expect to be _mostly correct_ on this data.\n",
    "* Next, we'll do the same thing but with the `test data,` i.e., data the model has never seen. We expect the correct prediction fraction to be less than the equivalent training value on the `test data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98398ccb-1ec6-40c7-9586-f3ad475011f1",
   "metadata": {},
   "source": [
    "### Compute the correct prediction fraction for the `training` and `test` datasets\n",
    "Process each image in the `testing_image_dataset` dataset. Pass the pixel data from the image into the `model` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels. If they argree, we update the `S` variable (a running count of the number of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c67a99f9-5efe-48de-a8f8-d9962dff24a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 85.97%\n"
     ]
    }
   ],
   "source": [
    "S_training = 0;\n",
    "for i ∈ eachindex(training_image_dataset)\n",
    "    \n",
    "    x = training_image_dataset[i][1];\n",
    "    y = training_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,number_digit_array)\n",
    "    y == ŷ ? S_training +=1 : nothing\n",
    "end\n",
    "correct_prediction_training = (S_training/length(training_image_dataset))*100;\n",
    "println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a021a683-8441-469d-8981-0c48800cf92b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction on the test data: 85.1%\n"
     ]
    }
   ],
   "source": [
    "S_testing = 0;\n",
    "for i ∈ eachindex(testing_image_dataset)\n",
    "    \n",
    "    x = testing_image_dataset[i][1];\n",
    "    y = testing_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    y == ŷ ? S_testing+=1 : nothing\n",
    "end\n",
    "correct_prediction_test = (S_testing/length(testing_image_dataset))*100;\n",
    "println(\"Correct prediction on the test data: $(correct_prediction_test)%\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
