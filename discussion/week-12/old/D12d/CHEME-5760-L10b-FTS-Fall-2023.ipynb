{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394ceef2-328a-476f-b5ea-1b8d80984b2b",
   "metadata": {},
   "source": [
    "## Example: Online Planning in the Lava Grid World - Forward Tree Search\n",
    "\n",
    "This example will familiarize students with the `rollout` solution of a `two-dimensional` navigation problem, i.e., the lava world [roomba](https://www.irobot.com) problem we have discussed. \n",
    "\n",
    "### Problem\n",
    "You have a [roomba](https://www.irobot.com) that has finished cleaning the kitchen floor and needs to return to its charging station. However, between your kitchen floor and the `charging station` (safety), there are one or more `lava pits` (destruction for the [roomba](https://www.irobot.com)). This is an example of a two-dimensional grid-world navigational decision task. \n",
    "\n",
    "This example will familiarize students with using `forward tree search` for solving a two-dimensional grid-world navigation task, the role of the discount factor $\\gamma$. In particular, we will:\n",
    "\n",
    "* __Task 1__: Build a `5` $\\times$ `5` world model with two lava pits and a charging station.\n",
    "* __Task 2__: Generate the components of the MDP problem \n",
    "* __Task 3__: Develop on online planning solution by implemeting the `forward_search(...)` routine of `Algorithm 9.2` of the [Decisions Book](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48e9efe-9565-402a-84aa-cc4f194930cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5760-Examples-F23`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Examples-F23/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57d202-434b-42ba-adef-0a3842a0ee4b",
   "metadata": {},
   "source": [
    "## Task 1: Build the world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's setup the data for the world, setup the states, actions, rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` that are avialble to the agent and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states, and setup the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564069a4-b30f-480f-83e3-7f72a3e651f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 5\n",
    "number_of_cols = 5\n",
    "nactions = 4;\n",
    "Œ≥ = 0.20;\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "ùíÆ = range(1,stop=nstates,step=1) |> collect;\n",
    "ùíú = range(1,stop=nactions,step=1) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fab8b7-67fe-4b3d-9cfa-a5c863540c22",
   "metadata": {},
   "source": [
    "Next, we'll set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. We only need to put `non-default` reward values in the reward dictionary (we'll add default values to the other locations later). Lastly, let's put the locations on the grid that are `absorbing`, meaning the charging station or lava pits in your living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f17787-9756-424c-8232-7cfdf4931a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup rewards -\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(2,2)] = -100000.0 # lava in the (2,2) square \n",
    "rewards[(4,4)] = -100000.0 # lava in the (4,4) square\n",
    "rewards[(3,3)] = 1000.0    # charging station square\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (2,2));\n",
    "push!(absorbing_state_set, (3,3));\n",
    "push!(absorbing_state_set, (4,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d5829-e0bb-488c-b2c4-05d4cc6aafa6",
   "metadata": {},
   "source": [
    "Finally, we can build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* We must pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ddd027-c480-4d63-a5b9-aa122e3aebb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = VLDecisionsPackage.build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80780e08-bac5-4b80-b73d-cc5c6dbe4bd7",
   "metadata": {},
   "source": [
    "## Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s‚Ä≤, a)`. Let's construct these from our grid world model instance, starting with the reward function `R(s, a)`:\n",
    "\n",
    "### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Œî`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13203c34-fd3e-4b19-8f48-825ce21d1460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25√ó4 Matrix{Float64}:\n",
       "  -50000.0       -1.0   -50000.0       -1.0\n",
       "  -50000.0  -100000.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0       -1.0\n",
       "  -50000.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0  -100000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0     1000.0  -100000.0       -1.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0       -1.0\n",
       " -100000.0       -1.0       -1.0     1000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0  -100000.0     1000.0       -1.0\n",
       "      -1.0       -1.0       -1.0   -50000.0\n",
       "      -1.0       -1.0   -50000.0       -1.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "    1000.0       -1.0       -1.0  -100000.0\n",
       "      -1.0       -1.0       -1.0       -1.0\n",
       "      -1.0       -1.0  -100000.0   -50000.0\n",
       "      -1.0   -50000.0   -50000.0       -1.0\n",
       "      -1.0   -50000.0       -1.0       -1.0\n",
       "      -1.0   -50000.0       -1.0       -1.0\n",
       " -100000.0   -50000.0       -1.0       -1.0\n",
       "      -1.0   -50000.0       -1.0   -50000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ‚àà ùíÆ\n",
    "    for a ‚àà ùíú\n",
    "        \n",
    "        Œî = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            if (haskey(rewards, new_position) == true)\n",
    "                R[s,a] = rewards[new_position];\n",
    "            else\n",
    "                R[s,a] = -1.0;\n",
    "            end\n",
    "        else\n",
    "            R[s,a] = -50000.0; # we are off the grid, big negative penalty\n",
    "        end\n",
    "    end\n",
    "end\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d472-a0c9-494b-b356-a2eb1cdd1b4e",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Œî`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s‚Ä≤,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b087127-75c8-40c6-b416-4ac41bd8081b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ‚àà ùíú\n",
    "    \n",
    "    Œî = world.moves[a];\n",
    "    \n",
    "    for s ‚àà ùíÆ\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Œî\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s‚Ä≤ = world.states[new_position];\n",
    "            T[s, s‚Ä≤,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bd54d-4a91-4431-bddb-b932b6654452",
   "metadata": {},
   "source": [
    "Finally, we construct an instance of the `MyMDPProblemModel` which encodes the data required to solve the MDP problem.\n",
    "* We must pass the states `ùíÆ`, the actions `ùíú`, the transition matrix `T`, the reward matrix `R`, and the discount factor `Œ≥` into the `build(...)` method. We store the MDP model in the `m` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe11305-9b82-4def-925c-99607990d6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = VLDecisionsPackage.build(MyMDPProblemModel, \n",
    "    (ùíÆ = ùíÆ, ùíú = ùíú, T = T, R = R, Œ≥ = Œ≥));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0aa32-4847-4ef5-999d-f403fea2ce11",
   "metadata": {},
   "source": [
    "## Task 3: Online planning solution: Forward Tree Search\n",
    "First, let's set the `depth` that are going to explore, i.e., how many steps are we going to take when exploring each state `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33d5d4de-39e8-4e24-a505-614413855418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b503d3-be44-4dcf-9e20-5194dbb0ef5f",
   "metadata": {},
   "source": [
    "Next, let's implement the functions:\n",
    "\n",
    "> The `mylookahead(p::MyMDPProblemModel, U::Function, s::Int64, a::Int64) -> Float64` function computes the immediate reward for taking action `a` in state `s`, and then the expected future utility using the state transition model $T(s,s^{\\prime},a)$ and the value (utility) function $U(s)$\n",
    "\n",
    "> The `forward_search(problem::MyMDPProblemModel,  s::Int64,  d::Int64, U::Function) -> NamedTuple` function computes the best action `a` starting at state `s` using a `forward search tree` method, where we explore the tree using a `depth-first` approach to a depth `d` \n",
    "\n",
    "These implementations were based on `Algorithm 9.12` of the [Decisions Book](https://algorithmsbook.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e69d7db7-d810-4aa5-89a3-75f734ea9abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mylookahead (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mylookahead(p::MyMDPProblemModel, U::Function, s::Int64, a::Int64)::Float64\n",
    "    \n",
    "    # println(\"mylookahead:starting ... with  s=$(s), a=$(a)\");\n",
    "    \n",
    "    # get data from the problem -\n",
    "    ùíÆ, T, R, Œ≥ = p.ùíÆ, p.T, p.R, p.Œ≥;\n",
    "    tmp = R[s,a] + Œ≥*sum(T[s, s‚Ä≤,a]*U(s‚Ä≤) for s‚Ä≤ in ùíÆ);\n",
    "    # println(\"mylookahead: finshed mylookup: s=$(s), a=$(a), value=$(tmp)\");\n",
    "    \n",
    "    return tmp;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc2fc656-2df9-467a-8617-a5e293c1519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function forward_search(problem::MyMDPProblemModel,  s::Int64,  d::Int64, U::Function)::NamedTuple\n",
    "    \n",
    "    if (d ‚â§ 0)\n",
    "        # println(\"forward_search: base condition L4: d = $(d), s=$(s) and u=$(U(s))\")\n",
    "        return (a=nothing, u=U(s))\n",
    "    end\n",
    "    best = (a=nothing, u=-Inf)\n",
    "    \n",
    "    # println(\"forward_search: main recursion L9 ... s=$(s), d=$(d) -> d=$(d-1)\")\n",
    "    U‚Ä≤(s) = forward_search(problem, s, d-1, U).u\n",
    "    # println(\"forward_search: starting to test actions L11: s=$(s), d=$(d) and U‚Ä≤(s) = $(U‚Ä≤(s))\")\n",
    "    for a ‚àà problem.ùíú\n",
    "        # println(\"testing action a = $(a)\");\n",
    "        u = mylookahead(problem, U‚Ä≤, s, a)\n",
    "        if (u > best.u)\n",
    "            best = (a=a,u=u)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # println(\"forward_search: completed action testing L20 with s=$(s), d=$(d): returning best=$(best)\")\n",
    "    return best\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "506b81a0-d315-4780-82a8-3c9adb500146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utest(s) = -1;\n",
    "# forward_search(m,1,2,Utest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b81f0-4857-49f9-bbe5-952074d7497e",
   "metadata": {},
   "source": [
    "### Compute the policy $\\text{my_}\\pi$\n",
    "Initialize a starting function $U(s)$, and then compute the best action `a` for each starting state `s` by calling the `forward_search(...)` function in a `for` loop iterating over states $s\\in\\mathcal{S}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35fccd61-8532-4bfb-a345-e7dedaabc894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] mylookahead(p::MyMDPProblemModel, U::var\"#U‚Ä≤#19\"{MyMDPProblemModel, Int64, typeof(U‚Çí)}, s::Int64, a::Int64)",
      "    @ Main ./In[32]:7",
      "  [2] forward_search",
      "    @ ./In[33]:14 [inlined]",
      "  [3] U‚Ä≤",
      "    @ ./In[33]:10 [inlined]",
      "  [4] (::var\"#17#18\"{var\"#U‚Ä≤#19\"{MyMDPProblemModel, Int64, typeof(U‚Çí)}, Int64, Int64, Array{Float64, 3}})(s‚Ä≤::Int64)",
      "    @ Main ./none:0",
      "  [5] MappingRF",
      "    @ ./reduce.jl:95 [inlined]",
      "  [6] _foldl_impl(op::Base.MappingRF{var\"#17#18\"{var\"#U‚Ä≤#19\"{MyMDPProblemModel, Int64, typeof(U‚Çí)}, Int64, Int64, Array{Float64, 3}}, Base.BottomRF{typeof(Base.add_sum)}}, init::Base._InitialValue, itr::Vector{Int64})",
      "    @ Base ./reduce.jl:62",
      "  [7] foldl_impl",
      "    @ ./reduce.jl:48 [inlined]",
      "  [8] mapfoldl_impl",
      "    @ ./reduce.jl:44 [inlined]",
      "  [9] #mapfoldl#288",
      "    @ ./reduce.jl:170 [inlined]",
      " [10] mapfoldl",
      "    @ ./reduce.jl:170 [inlined]",
      " [11] #mapreduce#292",
      "    @ ./reduce.jl:302 [inlined]",
      " [12] mapreduce",
      "    @ ./reduce.jl:302 [inlined]",
      " [13] #sum#295",
      "    @ ./reduce.jl:530 [inlined]",
      " [14] sum",
      "    @ ./reduce.jl:530 [inlined]",
      " [15] #sum#296",
      "    @ ./reduce.jl:559 [inlined]",
      " [16] sum(a::Base.Generator{Vector{Int64}, var\"#17#18\"{var\"#U‚Ä≤#19\"{MyMDPProblemModel, Int64, typeof(U‚Çí)}, Int64, Int64, Array{Float64, 3}}})",
      "    @ Base ./reduce.jl:559",
      "--- the last 16 lines are repeated 2 more times ---",
      " [49] mylookahead(p::MyMDPProblemModel, U::var\"#U‚Ä≤#19\"{MyMDPProblemModel, Int64, typeof(U‚Çí)}, s::Int64, a::Int64)",
      "    @ Main ./In[32]:7",
      " [50] forward_search",
      "    @ ./In[33]:14 [inlined]"
     ]
    }
   ],
   "source": [
    "U‚Çí(s) = 0;\n",
    "my_œÄ = Array{Int64,1}()\n",
    "for s ‚àà ùíÆ\n",
    "    a = forward_search(m, s, d, U‚Çí).a\n",
    "    push!(my_œÄ,a)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "724202a7-02c3-4a96-bea2-6a477a96c49f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Vector{Int64}:\n",
       " 2\n",
       " 4\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_œÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46295cc8-cdc8-4c23-833a-3d1152480083",
   "metadata": {},
   "source": [
    "Finally, we can extract the policy $\\pi(s)$ from the action-value function $Q(s,a)$ using the `policy(...)` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939025-1913-42e6-aa71-199c954588a1",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b341f84-a205-45da-8356-22a2eb6f9aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "move_arrows = Dict{Int,Any}();\n",
    "move_arrows[1] = \"‚Üê\"\n",
    "move_arrows[2] = \"‚Üí\"\n",
    "move_arrows[3] = \"‚Üì\"\n",
    "move_arrows[4] = \"‚Üë\"\n",
    "move_arrows[5] = \"‚àÖ\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01a16373-57e1-49ae-89e5-c9f365214e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) ‚Üí (2, 1)\n",
      "(1, 2) ‚Üë (1, 3)\n",
      "(1, 3) ‚Üí (2, 3)\n",
      "(1, 4) ‚Üí (2, 4)\n",
      "(1, 5) ‚Üí (2, 5)\n",
      "(2, 1) ‚Üí (3, 1)\n",
      "(2, 2) ‚àÖ\n",
      "(2, 3) ‚Üí (3, 3)\n",
      "(2, 4) ‚Üí (3, 4)\n",
      "(2, 5) ‚Üí (3, 5)\n",
      "(3, 1) ‚Üë (3, 2)\n",
      "(3, 2) ‚Üë (3, 3)\n",
      "(3, 3) ‚àÖ\n",
      "(3, 4) ‚Üì (3, 3)\n",
      "(3, 5) ‚Üì (3, 4)\n",
      "(4, 1) ‚Üê (3, 1)\n",
      "(4, 2) ‚Üê (3, 2)\n",
      "(4, 3) ‚Üê (3, 3)\n",
      "(4, 4) ‚àÖ\n",
      "(4, 5) ‚Üê (3, 5)\n",
      "(5, 1) ‚Üê (4, 1)\n",
      "(5, 2) ‚Üê (4, 2)\n",
      "(5, 3) ‚Üê (4, 3)\n",
      "(5, 4) ‚Üì (5, 3)\n",
      "(5, 5) ‚Üê (4, 5)\n"
     ]
    }
   ],
   "source": [
    "for s ‚àà ùíÆ\n",
    "    a = my_œÄ[s];\n",
    "    Œî = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    new_position =  current_position .+ Œî\n",
    "    \n",
    "    if (in(current_position, absorbing_state_set) == true)\n",
    "        println(\"$(current_position) $(move_arrows[5])\")\n",
    "    else\n",
    "        println(\"$(current_position) $(move_arrows[a]) $(new_position)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bf226-94d6-4376-a736-a650fa86db52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
